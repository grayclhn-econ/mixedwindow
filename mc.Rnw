\newcolumntype{C}{>{\centering\arraybackslash}X}
\SweaveOpts{prefix.string=floats/mc}
\SweaveOpts{split=TRUE}

<<echo=F>>=
library(dbframe)
library(xtable)
mcdata1 <- dbframe("dgp1", "mcdata.db")
mcdata2 <- dbframe("dgp2", "mcdata.db")
groups <- c( "isPower", "R", "P")
size <- .1

printbooktabs <- function(x,...){
  tab <- gsub("\\\\begin\\{tabularx\\}", "\\\\begin\\{tabularx\\}\\{\\\\textwidth\\}",
       print(xtable(x,...), file = "/dev/null", floating=F, hline.after=NULL, 
        add.to.row=list(pos=list(-1,0, nrow(x)), 
          command=c(
            '\\toprule ',
            '\\midrule ',
            '\\bottomrule ')),...))
  cat(tab)
  invisible(tab)
}

fspace <- function(x) gsub(" ", "\\\\enskip", format(round(x, 1)))

printall <- function(mcdb,...) {
  d <- subset(select(mcdb, group.by = groups, 
                     sprintf(c("case isPower when 0 then 'size' else 'power' end as 'type'",
                               "R", "P", 
                               "100 * avg(pOld <= %f) as 'Pr[\\textsc{cw}]'",
                               "100 * avg(pNew <= %f) as 'Pr[new]'",
                               "100 * (avg(pOld <= %f and pNew > %f) + avg(pOld > %f and pNew <= %f)) as 'Pr[disagree]'"),
                             size, size, size, size)), select = -isPower)
  d$R <- fspace(d$R)
  d$P <- fspace(d$P)
  d[[4]] <- fspace(d[[4]])
  d[[5]] <- fspace(d[[5]])
  d[[6]] <- fspace(d[[6]])
  d$R[duplicated(subset(d, select = c(type, R)))] <- NA
  d$type[duplicated(d$type)] <- NA
  printbooktabs(d, include.rownames = FALSE, digits = c(0,0,0,0,1,1,1), 
                align = c("l","X","C","C","C","C","C"), 
                sanitize.text.function = function(x) x,
                tabular.environment = "tabularx")
  invisible(d)
}
@ 

I'll conduct two brief Monte Carlo experiments to demonstrate that the
modified version of \citepos{ClW:07} statistic performs similarly to
their original test.\footnote{All of these simulations were programmed
  in R \citep{R:2-13-0} and use the \textsc{MASS} \citep{VeR:02},
  xtable \citep{Dah:09}, dbframe \citep{Cal:10b}, and \textsc{oos}
  \citep{Cal:11b} packages.  Version information is listed in the
  bibliography.}  We'll use both of the \dgp s they consider.  For the
first, we have
\begin{align}\label{eq:dgp1}\tag{\dgp\ 1}
  y_t &= 0.5 + \gamma^{*} z_{t-1} + e_t &
  \gamma^{*} &=
  \begin{cases}
    0 & \text{size simulations} \\
    0.35 & \text{power simulations}
  \end{cases}\\\nonumber
  z_t &= 0.15 + 0.95 z_{t-1} + v_t &
  (e_t, v_t)' &\sim iid\ N\Bigg(\begin{pmatrix} 0 \\ 0
  \end{pmatrix}
   , \begin{pmatrix} 18 & -
    0.5 \\ -0.5 & 0.025 \end{pmatrix}\Bigg)
\end{align}
Both models are estimated by \ols. The benchmark model regresses $y_t$
on a constant, and the alternative regresses $y_t$ on a constant and
$z_{t-1}$.  We use both \poscw\ original statistic and the new
statistic to test the null that the benchmark model's innovation is an
\mds.\footnote{\citet{ClW:07} report the performance of the tests
  proposed by \citet{CCS:01} and \citet{ClM:05} as well, and of tests
  based on the naive Gaussian statistic.}


\begin{table}[tb]
  \centering
<<mc1, echo=F, results=tex>>=
printall(mcdata1)
@ 
\caption{Size and power of the \oos\ tests under \eqref{eq:dgp1} at
  10\% confidence.  These percentages are calculated from 5000
  samples.  Pr[\textsc{cw}] shows the fraction of simulations for which Clark
  and West's (2007) statistic rejects; Pr[new] shows the fraction of
  simulations for which this paper's test rejects; and Pr[disagree]
  gives the fraction of simulations in which this paper's test and
  Clark and West's (2007) give different conclusions.}
\label{tab:mc1}
\end{table}

The second \dgp\ is
\begin{align} \label{eq:dgp2}\tag{\dgp\ 2}
  y_{t} &= 2.237 + 0.261 + \gamma^{*}_{1} z_{t-1} + \gamma_{2}^{*}
  z_{t-2} + \gamma_{3}^{*} z_{t-3} + \gamma_{4}^{*} z_{t-4} + e_{t} \\
  z_{t} &= 0.804 z_{t-1} - 0.221 z_{t-2} + 0.226 z_{t-3} - 0.205
  z_{t-4} + v_{t} \nonumber \\
  \binom{e_{t}}{v_{t}} &\sim N\Bigg(
  \begin{pmatrix}
    0 \\ 0
  \end{pmatrix},
  \begin{pmatrix}
    10.505 & 1.036 \\ 1.036 & 0.366
  \end{pmatrix}
  \Bigg) \nonumber\\\nonumber
  \gamma^{*} &=
  \begin{cases}
    0 & \text{size simulations}\\
    (3.363, -0.633, -0.377, -0.529)' & \text{power simulations}.
  \end{cases}
\end{align}
The benchmark model for $y_{t}$ is an AR(1) and the alternative model
includes all four lags of $z_{t}$.  For this simulation, $R$ is 80 or
120 and $P$ is 40, 80, 120, or 160.  \citet{ClW:07} argue that the
first \dgp\ mimics an asset pricing application and the second mimics
\gdp\ growth forecasting using the Federal Reserve Bank of Chicago's
National Activity Index.

\begin{table}[tb]
  \centering
<<mc2, echo=F, results=tex>>=
printall(mcdata2)
@ 
\caption{Size and power of the \oos\ tests under \eqref{eq:dgp2} at
  10\% confidence.  These percentages are calculated from 5000
  samples.  Pr[\textsc{cw}] shows the fraction of simulations for which Clark
  and West's (2007) statistic rejects; Pr[new] shows the fraction of
  simulations for which this paper's test rejects; Pr[disagree] gives
  the fraction of simulations in which this paper's test and Clark and
  West's (2007) give different conclusions.}
\label{tab:mc2}
\end{table}

Table~\ref{tab:mc1} presents the simulation results
for~(\ref{eq:dgp1}) and Table~\ref{tab:mc2} for~(\ref{eq:dgp2}).  For
all of the parameter values, the proposed new statistic has similar
rejection probability to \citepos{ClW:07}.  Clark and West's test is
slightly more undersized than the new test, but has a little higher
power under the alternative.  Moreover, the statistics give different
results in less than 10\% of the simulations for most parametrizations
of the \dgp s.  In general, the statistics perform similarly well.

% This table presents results for an auxiliary simulation that has R
% very low and P very high, and basically works as a check

% \begin{table}[tb]
%  \centering
% <<mc3, echo=F, results=tex>>=
% % mcdata3 <- dbframe("dgp3", "mcdata.db")

% d <- subset(select(mcdata3, group.by = c("R", "P"), 
%                    sprintf(c("R", "P", 
%                              "100 * avg(pOld <= %f) as 'Pr[\\textsc{cw}]'",
%                              "100 * avg(pNew <= %f) as 'Pr[new]'",
%                              "100 * (avg(pOld <= %f and pNew > %f) + avg(pOld > %f and pNew <= %f)) as 'Pr[disagree]'"),
%                            size, size, size, size)))
% d$R <- fspace(d$R)
% d$P <- fspace(d$P)
% d[[3]] <- fspace(d[[3]])
% d[[4]] <- fspace(d[[4]])
% d[[5]] <- fspace(d[[5]])
% printbooktabs(d, include.rownames = FALSE, digits = c(0,0,0,1,1,1), 
%               align = c("l","C","C","C","C","C"), 
%               sanitize.text.function = function(x) x,
%               tabular.environment = "tabularx")
% @ 
% \caption{Size and power of the \oos\ tests under sanity-check \dgp\ at
%   10\% confidence.  These percentages are calculated from 5000
%   samples.  Pr[\textsc{cw}] shows the fraction of simulations for which Clark
%   and West's (2007) statistic rejects; Pr[new] shows the fraction of
%   simulations for which this paper's test rejects; Pr[disagree] gives
%   the fraction of simulations in which this paper's test and Clark and
%   West's (2007) give different conclusions.}
% \label{tab:mc3}
% \end{table}
% Table~\ref{tab:mc3} presents one more simulation; the alternative
% model is $y_{t} = 0 + 0 x_{1t} + 0 x_{2t} + \varepsilon$ and the
% benchmark model is just a constant.  This simulation is meant as a
% sanity check on the previous results.

%%% Local Variables:
%%% TeX-master: "Paper"
%%% End:
% LocalWords:  dbframe xtable mcdata dgp printbooktabs hline pos nrow toprule
% LocalWords:  midrule bottomrule printall mcdb dsize isPower sprintf pOld pNew
% LocalWords:  dpower qs dadj dfapply rownames tb tex oos eqref eq mc ClW dgp's
% LocalWords:  citepos nonumber sim iid Bigg binom pmatrix ols poscw CCS gsub
% LocalWords:  ClM tabularx textwidth fspace enskip VeR Dah
