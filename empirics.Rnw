\newcolumntype{C}{>{\centering\arraybackslash}X}
\SweaveOpts{prefix.string=floats/empirics}
\SweaveOpts{split=TRUE}

<<echo=F>>=
library(xtable)
library(Hmisc)
load("empirics.RData")
attach(results)

printbooktabs <- function(x,...){
  tab <- gsub("\\\\begin\\{tabularx\\}", "\\\\begin\\{tabularx\\}\\{\\\\textwidth\\}",
       print(xtable(x,...), file = "/dev/null", floating=F, hline.after=NULL, 
        add.to.row=list(pos=list(-1,0, nrow(x)), 
          command=c(
            '\\toprule ',
            '\\midrule ',
            '\\bottomrule ')),...))
  cat(tab)
  invisible(tab)
}

fspace <- function(x, rnd=1) gsub("-", "--", gsub(" ", "\\\\enskip", format(round(x, rnd))))
@ 

This section demonstrates the use of the statistics introduced earlier
in the paper by revisiting \citepos{GoW:08} study of excess stock
returns.  Goyal and Welch argue that many variables thought to predict
excess returns (measured as the difference between the yearly log
return of the S\&P 500 index and the T-bill interest rate) on the
basis of in-sample evidence fail to do so out-of-sample.  To show
this, Goyal and Welch look at the forecasting performance of models
using a lag of the variable of interest, and show that these models do
not significantly outperform the excess return's recursive sample mean.

Here, I conduct the same analysis,, but using this paper's \mds\ test.
The benchmark model is the excess return's sample mean (as in the
original) and the alternative models are of the form
\[\text{excess return}_{t} = \alpha_{0} + \alpha_{1}\ 
\text{predictor}_{t-1} + \varepsilon_{t},\]
where $\alpha_{0}$ and $\alpha_{1}$ are estimated by \ols\ using a
10-year window.  The predictors used are listed in the ``predictor''
column of Table~\ref{tab:em1} \citep[see][for a detailed description
of the variables]{GoW:08}.  We also consider \citepos{CaT:08} proposed
correction to the models, that the forecasts be bounded below by zero
since negative forecasts are incredible, as well as two simple
combination forecasts, the mean and the median (over both the original
and the non-negative forecasts).  The data set is annual data
beginning in 1927 and ending in 2009, and the rolling window is of
length 10.

Table~\ref{tab:em1} presents the results for each model.  The column
``value'' gives the value of the test statistic for each model, while
the ``naive'' and ``corrected'' columns indicate whether the statistic
is greater than the standard size-10\% critical value
(\Sexpr{round(qnorm(.9),2)}) and the critical value estimated by the
procedure of Theorem~\ref{res:2}
(\Sexpr{round(sqrt(crit),2)}).\footnote{The bootstrap uses 999
  replications, and we use the i.i.d. bootstrap proposed in
  Part~\ref{it:1} of Theorem~\ref{res:2}.}  Three predictors are
significant at the naive critical values for both the original and
bounded forecasts: the dividend yield, long term interest rate, and
book to market ratio.  But none are significant after accounting for
data-snooping, which highlights the importance of these methods.  The
median forecast is significant using conventional critical values as
well, but not the corrected values.

\begin{table}[tb!]
  \centering
<<table, echo=F, results=tex>>=
k <- length(tstats)/2 - 1
d <- data.frame(notes = c("Original", rep("", k-1), "CT correction", rep("", k-1), "Combination", ""),
                predictor = names(tstats),
                tstat = fspace(tstats, 2),
                naive = ifelse(tstats > qnorm(.9), "sig.", ""),
                smart  = ifelse(tstats > crit, "sig.", ""),
                row.names = 1:(2*k + 2))
colnames(d)[1] <- " "
colnames(d)[3] <- "value"
colnames(d)[4] <- "naive"
colnames(d)[5] <- "corrected"

d$predictor <- gsub("\\.mu", "", d$predictor)
d$predictor <- gsub("\\.CT", "", d$predictor)
d$predictor <- gsub("\\.", " ", d$predictor)
d$predictor <- capitalize(d$predictor)


printbooktabs(d, sanitize.text.function = function(x) x, include.rownames = FALSE,
              align = c("l", "l", "l", rep("C", 3)), tabular.environment = "tabularx")
@ 
\caption{Results from \oos\ comparison of equity premium prediction
  models; the benchmark is the recursive sample mean of the equity
  premium and each alternative model is a constant and single lag of
  the variable listed in the ``predictor'' column.  The dataset begins
  in 1927 and ends in 2009 and is annual data. The ``value'' column
  lists the value of this paper's \oos\ statistic, the ``naive''
  column indicates whether the statistic is significant at standard
  critical values, and the ``corrected'' column indicates significance
  using the critical values proposed in Theorem~\ref{res:2} that
  account for the number of models.  See Section~\ref{sec:3} for details.}
\label{tab:em1}
\end{table}

%%% Local Variables:
%%% TeX-master: "Paper"
%%% End:
% LocalWords:  dbframe xtable mcdata dgp printbooktabs hline pos nrow toprule
% LocalWords:  midrule bottomrule printall mcdb dsize isPower sprintf pOld pNew
% LocalWords:  dpower qs dadj dfapply rownames tb tex oos eqref eq mc ClW dgp's
% LocalWords:  citepos nonumber sim iid Bigg binom pmatrix ols poscw CCS Hmisc
% LocalWords:  ClM empirics RData gsub tabularx textwidth fspace rnd enskip GoW
% LocalWords:  Goyal Welch return's CaT qnorm crit tstats tstat ifelse sig sqrt
% LocalWords:  colnames
