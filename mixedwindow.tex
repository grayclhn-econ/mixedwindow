\documentclass[12pt,fleqn]{article}
\input{tex/setup}
\input{tex/mcDef}
\input{tex/ap}

% These commands are generated when the Monte Carlo and applied
% sections are run; I'm giving them definitions now so that LaTeX will
% compile the document even if that code hasn't been run yet.
\providecommand\bootsize{[missing]}
\providecommand\empiricalcriticalvalue{[missing]}
\providecommand\nboot{[missing]}
\providecommand\nmod{[missing]}
\providecommand\testsize{[missing]}
\providecommand\totalsims{[missing]}
\providecommand\windowlength{[missing]}
\providecommand\empiricaltable{[missing]}
\providecommand\empiricaldraws{[missing]}
\providecommand\naivecriticalvalue{[missing]}

\providecommand\phantomsection{}

\author{Gray Calhoun\thanks{Economics Department; Iowa State
    University; Ames, IA 50011.  Telephone: (515) 294-6271.  Email:
    \guillemotleft \protect\url{gcalhoun@iastate.edu}\guillemotright,
    web: \guillemotleft http://gray.clhn.org\guillemotright.
    If you find errors or mistakes in this paper, please email me or
    open a new issue at
    \guillemotleft https://github.com/grayclhn/mixedwindow/issues\guillemotright.
    I'd like to
    thank Helle Bunzel, Todd Clark, Graham Elliott, Yu-Chin Hsu,
    Michael McCracken, Pablo Pincheira, Elie Tamer, Allan Timmermann, Ken West, Stephane
    Meng-Feng Yen, several referees, and participants at the 2011 Midwest Econometrics
    Group meeting and the 2013 \nber-\nsf\ Time Series conference
    for helpful comments and discussions.  I'd also like to thank Amit
    Goyal for providing computer code and data for his 2008
    RFS paper with Ivo Welch \citep{GoW:08}. An earlier version of this paper
    circulated under the name ``An asymptotically normal out-of-sample
  test of equal predictive accuracy for nested models.''}\\%
  Iowa State University}

\title{An asymptotically normal out-of-sample test based on
  mixed estimation windows}

\begin{document}
\maketitle

\addcontentsline{toc}{section}{Abstract}
\begin{abstract}
  \noindent This paper develops a modification of \citepos[\textit{J.
    Econom.}]{ClW:07} adjusted out-of-sample $t$-test. We propose
  using a recursive window to estimate the benchmark model but a
  fixed-length rolling window to estimate the alternative. The
  resulting statistic is asymptotically normal even when the models
  are nested. The
  paper also presents Monte Carlo evidence that this statistic has
  much higher power than existing out-of-sample statistics in a common
  use-case for these tests: when the \dgp\ is subject to instability.
  This procedure is then used to analyze
  \citepos[\textit{Rev. Finan. Stud.}]{GoW:08} excess returns dataset
  and supports their finding that the equity premium is unpredictable
  out-of-sample.

\strut

\noindent Keywords: Forecast Evaluation, Martingale Difference
Sequence, Model Selection, Forecast Encompassing

\strut

\noindent \allcaps{JEL} Classification Numbers: C22, C53

\end{abstract}
\newpage
\markboth{}{}
\tableofcontents
\newpage

\section{Introduction}

This paper proposes an out-of-sample (\oos) test statistic that is
asymptotically normal and correctly centered even when the models
studied are nested. The test is based on one proposed by
\citet{ClW:06,ClW:07}, but we propose estimating the benchmark model
with a recursive window and the alternative model with a fixed length
rolling window. The rolling window ensures asymptotic normality, as in
\citet{GiW:06}, and the recursive window allows the null hypothesis to
be a statement about the specification of the Data Generating Process,
which is the focus of the vast majority of the \oos\ testing
literature.%
\footnote{In particular, this is the focus of \citet{Wes:96},
  \citet{ClM:01}, \citet{Mcc:07}, \citet{ClW:06}, \citet{ClW:07}, and
  many others but not \citet{GiW:06}. See \citet{Wes:06} and
  \citet{ClM:13} for a thorough overview of this literature.} %
This combination of estimation windows also gives our test statistic
high power against alternatives that cause the benchmark model
to be unstable --- structural breaks, time-varying coefficients, or
forms of nonlinearity, for example --- which is a common motivation
for using these tests.

\oos\ tests are common in International Macroeconomics,
Macroeconomics, and Finance (see, for example, \citealt{MeR:83};
\citealt{StW:03}; and \citealt{GoW:08}) and there is a substantial
literature developing the theoretical properties of these statistics,
beginning primarily with \citet{DiM:95} and
\citet{Wes:96}.
In a pair of papers,
\citet{ClW:06,ClW:07} develop an \oos\ test of the null hypothesis
that a small benchmark model is correctly specified.  Their test
compares the forecasting performance of a pair of nested models, and
the null hypothesis is that the innovations in the smaller model form
a Martingale Difference Sequence (\mds).  This test procedure is popular, and
one assumes that this is due in part to the statistic's convenience,
the statistic is approximately normal after adjusting for the
estimation error of the larger model.  Normality comes from a
fixed-length rolling window, as in \citet{GiW:06}, and the adjustment
centers the statistic to have mean-zero under the null.  This statistic is especially
convenient because other \oos\ tests for similar hypotheses
(\citealt{CCS:01}; \citealt{ClM:01,ClM:05}; \citealt{CoS:02,CoS:04};
and \citealt{Mcc:07}; among others) have a nonstandard limit
distribution and place restrictions on the models under consideration,
while other asymptotically normal statistics test a different null
hypothesis \citep{GiW:06} or place assumptions on the models and \dgp\
that are often violated in empirical work (\citealt{DiM:95};
\citealt{Wes:96}; \citealt{WeM:98};
\citealt{Mcc:00}).%
\footnote{\citet{DiM:95} assume that the models are
  not estimated. \citet{Wes:96}, \citet{WeM:98}, and \citet{Mcc:00}
  assume that the models do not converge to the same limit,
  which rules out nesting.} %
However, Clark and West's statistic is only ``approximately normal''
in an informal sense.  Clark and West present Monte Carlo evidence of
the statistic's distribution, but only prove that the statistic is
asymptotically normal with mean zero when the benchmark model is
not estimated \citep{ClW:06}. Estimating the parameters of the smaller
model invalidates their proof.

This paper proposes a modified version of Clark and West's (2006,
2007) statistic and shows that it is
asymptotically normal even when the smaller model is estimated.  To
achieve normality, the pseudotrue benchmark model must be estimated
consistently, but the larger alternative model must continue to be
estimated inconsistently so that the test statistic is not degenerate
when the models are nested. We can meet both needs by
using different window strategies for each model: the benchmark model
is estimated using a recursive window and the alternative with a
fixed-length rolling window.
This approach has the further advantage over existing \oos\ tests for
nested models that the alternative can be essentially arbitrary as
long as high level moment conditions hold. In particular, researchers
can use model selection techniques like the \aic\ or \bic\ to
determine the number of lags to include, the particular exogenous
variables to include, etc. Moreover, although we focus on nested
models in this paper, the approach can be used with non-nested models
as well. As \citet{ClM:11b} have recently argued,
\citepos{Wes:96} results do not hold when the true \dgp\ is nested by
the benchmark and alternative models, which is allowable under the
null hypothesis of interest. (\citealp{ClM:11b}, call this scenario ``overlapping models.'')

The next section presents the intuition and theory for our new
statistic.  Section~\ref{sec:2} presents simulations that compare our
pairwise \oos\ test to \poscw\ original statistics.
Section~\ref{sec:3} demonstrates the use of our statistic by
reanalyzing \citepos{GoW:08} study of excess return predictability and
demonstrates how our results can be used in settings with many
alternative models. Section~\ref{sec:4} concludes. Our results follow
from arguments similar to \citepos{Wes:96} and have been put in a
separate appendix along with some supporting lemmas \citep{Cal:15b}.

\section[Asymptotic normality]{Theoretical results supporting the asymptotically normal \oos\ statistic}
\label{sec:1}

This section presents the new \oos\ statistic; first we give an
informal motivation of the statistic, then present the paper's key
assumptions in Section~\ref{sec:1a} and present our formal theoretical
results in Section~\ref{sec:1b}.

Suppose for now that a researcher is interested in
predicting the target variable $y_{t+1}$ with a vector of regressors
$x_t$, that $v_t$ is another random process that is believed to
potentially contain information about $y_{t+1}$, and that
$(y_t, x_t, v_t)$ is stationary and weakly dependent.
In addition,
let $\btrue = (\E x_t x_t')^{-1} \E x_t y_{t+1}$ be the pseudotrue
coefficient for the regression of $y_{t+1}$ on $x_t$ and define
$\ep_{t+1} = y_{t+1} - x_t'\btrue$.  If this linear model is
correctly specified, then $\ep_{t+1}$ is an \mds\ with respect
to $\sigma((x_t, v_t, y_t), (x_{t-1}, v_{t-1}, y_{t-1}),\dots)$
and we can see immediately that
\begin{equation}
  \label{eq:4}
  \oclt{t} \ep_{t+1} (v_t - x_t'\btrue)
\end{equation}
obeys an \mds\ \clt\ and is asymptotically normal as $P \to \infty$,%
\footnote{This claim assumes that the asymptotic variance of the
  sample average is uniformly positive, a requirement that we will
  address in Section~\ref{sec:1b}.} %
with $R$ an arbitrary starting value
and $P = T - R$.

Straightforward algebra \citep{ClW:07} shows that
\begin{equation}
  \label{eq:5}
  \tfrac{1}{\sqrt{P}} \osum{t} \ep_{t+1} (v_t -
  x_t'\btrue) = \tfrac{1}{2 \sqrt{P}} \osum{t} \Big[(y_{t+1} -
  x_t\btrue)^2 - (y_{t+1} - v_t)^2 + (x_t'\btrue - v_t)^2 \Big]
\end{equation}
almost surely.
\citet{ClW:06,ClW:07} base their \oos\ statistics on the \allcaps{RHS} of
Equation~\eqref{eq:5}, but use a second forecast of $y_{t+1}$ as
$v_t$. (Call it $\yh_{t+1}$.) They use a rolling window of length
$R$ to estimate $\yh_{t+1}$,%
\footnote{Making $\yh_{t+1}$ a function of $y_t, x_{t-1}, z_{t-1}
\dots, y_{t-R+1}, x_{t-R}$ and $z_{t-R}$, where $z_t$ is another weakly
dependent random process} %
and $R$ is kept finite as $T \to \infty$ so that
$\yh_{t+1}$ inherits the weak dependence properties of the
variables used to estimate it. Using a finite window prevents
the degeneracy that can arise when comparing nested models out-of-sample (see
\citealp{ClM:01}, and \citealp{Mcc:07}), so the conditional variance
of the \oos\ average remains positive and the average obeys a \clt.%
\footnote{This approach was first introduced by \citet{GiW:06}.} %

\citet{ClW:06,ClW:07} propose using this as a test of whether the benchmark is correctly specified.
In their 2006 paper, Clark and West assume that the
coefficients on the benchmark model, $\btrue$, are zero under the null, making
$\ep_{t+1}$ observed directly. This restriction is relaxed in
their 2007 paper, where $\btrue$ is unknown and estimated with the same length-$R$
rolling window as $\yh_{t+1}$. Now the estimated linear model's prediction errors,
$\eph_{t+1}$, replace $\ep_{t+1}$ in the \oos\
test statistic. Unfortunately, $\eph_{t+1}$ is not an
\mds\ even when $\ep_{t+1}$ is, so the statistic is no longer
asymptotically mean-zero normal, even though this approximation
performs well in simulations. Since the window length is finite,
the estimator of $\btrue$ does not converge to $\btrue$.

This paper proposes using the same basic \oos\ statistic,
but using a recursive window to estimate $\btrue$ and produce
$\eph_{t+1}$:
\begin{align}
  \label{eq:8}
  \bh_t &= \Big(\sum_{s=1}^{t-1} x_{s} x_{s}'\Big)^{-1}
  \sum_{s=1}^{t-1} x_{s} y_{s+1}
  && \text{and}
  &
  \eph_{t+1} &= y_{t+1} - x_t'\bh_t
\end{align}
for each $t$.%
\footnote{The matrix inversion in $\bh_t$ can be replaced with a
  pseudo-inverse if necessary for some values of $t$ without changing
  the forecast.} %
\citepos{Wes:96} Theorem 4.1 implies that
\begin{equation*}
  \oclt{t} \Big[(y_{t+1} -
  x_t\bh_t)^2 - (y_{t+1} - v_t)^2 + (x_t'\bh_t - v_t)^2 \Big]
\end{equation*}
is asymptotically normal with mean zero under Clark and West's
\mds\ null for fairly
arbitrary processes $v_t$, as long as $v_t$ is weakly dependent and
the \oos\ statistic has uniformly positive variance.  Just as in
\citet{ClW:06,ClW:07}, these conditions are ensured if $v_t$ is
another forecast of $y_{t+1}$ based on a fixed-length rolling window.

So far, we have presented an especially simple version of the result
to make the intuition as clear as possible. The next section lists the
specific assumptions for the more general case and defines additional notation.

\subsection{Theoretical assumptions}
\label{sec:1a}

Consider the following environment. There is a single linear
benchmark model of the target variable, $y_{t+1}$:
\begin{equation}\label{eq:1}
  y_{t+1} = x_t'\beta + \ep_{t+1}, \quad t = 1,\dots,T-1
\end{equation}
where $\beta$ is an unknown vector of parameters and $x_t$ is an
observed vector of predictors. The parameter $\beta$ is estimated with
\ols\ using a recursive window as described by Equation~\eqref{eq:8}.
The alternative model is denoted $\yh_{t+1}$ and is estimated with a
rolling window of length $R$.

The main conditions on the \dgp\ are summarized in the first
assumption.  The weak dependence and moment conditions are
standard. The assumption of strict stationarity is stronger than
necessary in practice --- once the alternative forecasting method is
known, it is only necessary that the \oos\ adjusted loss difference be
weak stationary, and even that can be relaxed further --- but this
stronger assumption ensures that the results hold generally.

\phantomsection
\addcontentsline{toc}{subsubsection}{Assumption \ref{a1}}
\begin{asmp}\label{a1}%
  The data are generated by the relationship
  \begin{equation}
    y_{t+1} = x_t'\btrue + \ep_{t+1}
  \end{equation}
  for $t=1,2,\dots$, for some value $\btrue$, with $\E x_t \ep_{t+1} =
  0$, $\E \ep_{t+1}^2 > 0$, and $\E x_t x_t'$ positive definite for
  all $t$. Also assume that there is an additional sequence of random
  vectors $z_t$ and the process $(\ep_{t+1}, x_t, z_t)$ is stationary
  and strong mixing of size $-r/(r-2)$ or uniform mixing of size
  $-r/(2r-2)$, for $r > 2$.
\end{asmp}

The next assumption defines the forecasting models and adds additional
constraints to the \dgp.

\phantomsection
\addcontentsline{toc}{subsubsection}{Assumption \ref{a3}}
\begin{asmp}\label{a3}%
  The benchmark forecast is $x_t'\bh_t$, where $\bh_t$ is constructed
  with a recursive window according to~\eqref{eq:8}. The alternative
  forecast satisfies
  \begin{equation}
    \yh_{t+1} = \psi(y_t,z_t,\dots,y_{t-R+1}, z_{t-R+1})
  \end{equation}
  where $\psi$ is a known measurable function and the window length,
  $R$, remains finite as $T \to \infty$. Moreover, the vector
  $(\ep_{t+1}, x_t, \yh_{t+1})$ has uniformly bounded $2 r$ moments
  where $r$ is first defined in Assumption~\ref{a1}.
\end{asmp}

The requirement that the alternative forecast satisfies moment
conditions, rather than the underlying predictors $z_t$, is somewhat
unappealing but necessary. The function $\psi$ that generates these
forecasts is otherwise nearly unrestricted, so even well-behaved predictors
could produce arbitrarily badly-behaved forecasts. For example, if
\begin{equation*}
  z_t \sim \iid~\bernoulli(1/2),
\end{equation*}
setting $\psi(y_t, z_t) = 1/z_t$ would prevent a \clt\ from holding
since the forecast equals positive infinity with probability $1/2$. It
is easy to construct less obvious examples of problematic functions as
well. Assumption~\ref{a3} implicitly rules out these functional forms
by imposing moment conditions on the alternative models' forecasts.

Our next assumption ensures that the asymptotic variance of the \oos\
average is positive.
\phantomsection
\addcontentsline{toc}{subsubsection}{Assumption \ref{a4}}
\begin{asmp}\label{a4}%
  The asymptotic variance-covariance matrix
  \begin{equation}
    \var \Bigg(
      \oclt{t} \begin{pmatrix} x_t \\ \yh_{t+1} \end{pmatrix} \ep_{t+1}
      \Bigg)
  \end{equation}
  is uniformly positive definite (in $T$).
\end{asmp}
This assumption is much less restrictive than in \cite{Wes:96}.  As in
\cite{GiW:06} and \citet{ClW:06,ClW:07}, the assumption only serves to
rule out pathological cases --- for example, letting the alternative
model consist of only the first regressor of the benchmark. In \citet{Wes:96}, this
assumption is a restriction on the \dgp\ as well as the forecasting
models, but in this paper it is a restriction only on the models.

The final assumption restricts the class of \hac\ variance estimators
we will consider. We use the same class of estimators studied by
\citet{JoD:00} (their class $\mathcal{K}$); see their paper for
further discussion.
\phantomsection
\addcontentsline{toc}{subsubsection}{Assumption \ref{a5}}
\begin{asmp}\label{a5}%
  The kernel $K$ is a function from $\Re$ to $[-1,1]$ such that $K(0) = 1$, $K(x)
  = K(-x)$ for all $x$, $K(\cdot)$ is continuous at zero and all but a
  finite number of points, and
  \begin{gather*}
    \int_{-\infty}^{\infty} \lvert K(x) \rvert\, dx < \infty,
    \intertext{and}
    \int_{-\infty}^{\infty} \Bigg\lvert
    \int_{-\infty}^{\infty} K(z) e^{ixz}\,dz \Bigg\rvert\, dx < \infty.
  \end{gather*}
\end{asmp}

Last, we define some notation that will be used to derive the
theoretical properties of our \oos\ statistics.  The information set
that contains the information available for forecasting $y_{t+1}$ is
\begin{equation*}
  \Fs_t = \sigma(y_t, x_t, z_t, y_{t-1}, x_{t-1}, z_{t-1},\dots).
\end{equation*}
The adjusted \oos\ loss difference using a hypothetical value of
$\beta$ to produce the benchmark forecast is denoted by
\begin{equation*}
  f_t(\beta) = (y_{t+1} - x_t'\beta)^2 - (y_{t+1} - \yh_{t+1})^2 + (x_t'\beta - \yh_{t+1})^2.
\end{equation*}
Define the additional terms $\fh_t = f_t(\bh_t)$, $f_t = f_t(\btrue)$,
\begin{gather*}
  \gh_t = 2 \Bigg[\oavg{s} (x_s'\bh_s - \yh_{s+1}) x_s'\Bigg]\,
          \Bigg[\tfrac{1}{T-1} \sum_{s=1}^{T-1} x_s x_s'\Bigg]^{-1} x_t \eph_{t+1}
  \intertext{and}
  g_t = 2 \E\Big[(x_t'\btrue - \yh_{t+1}) x_t'\Big] \, (\E x_t x_t')^{-1} x_t \ep_{t+1}
\end{gather*}
and the \oos\ averages $\fb = \osum{t} \fh_t/P$, $\fb^* = \osum{t}
f_t/P$, $\gb = \osum{t} \gh_t/P$, and $\gb^* = \osum{t} g_t/P$.

\subsection{Theoretical results}
\label{sec:1b}

Asymptotic normality of the \oos\ average now follows directly from the
first three assumptions without other conditions. The proof is
presented in the Appendix and follows \citet{Wes:96} closely.

\phantomsection
\addcontentsline{toc}{subsubsection}{Theorem \ref{res:1}}
\begin{thm}\label{res:1}\input{mixedwindow_thm1}\end{thm}

To use this result, we need a consistent estimator of
$\sigma^2$. Define the \hac\ covariance estimator $\sigmah^2_1 =
\sh_{11} + 2 (\sh_{12} + \sh_{13})$ and the \mds\ covariance estimator
$\sigmah^2_2 = \sh_{21} + 2(\sh_{22} + \sh_{23})$ with
\begin{align*}
  \sh_{11} &= \oavg{s,t} (\fh_s - \fb) (\fh_t - \fb) K(\tfrac{t-s}{P}), &
  \sh_{21} &= \oavg{t} (\fh_t - \fb)^2, \\
  \sh_{12} &= \oavg{s,t} (\fh_s - \fb)(\gh_t - \gb) K(\tfrac{t-s}{P}), &
  \sh_{22} &= \oavg{t} (\fh_t - \fb)(\gh_t - \gb),
\intertext{and}
  \sh_{13} &= \oavg{s,t} (\gh_s - \gb) (\gh_t - \gb), &
  \sh_{23} &= \oavg{t} (\gh_t - \gb)^2.
\end{align*}

These estimators are consistent under similar assumptions to
Theorem~\ref{res:1}.

\phantomsection
\addcontentsline{toc}{subsubsection}{Lemma \ref{lem:2}}
\begin{lem}\label{lem:2}\input{mixedwindow_lem2}\end{lem}

Note that these results allow misspecification; asymptotic normality
follows from the weak dependence of the underlying series and from the
design of the test statistic. These statistics have typically been
used to test the null hypothesis that the benchmark model is correctly
specified --- that $\{\ep_t, \Fs_t\}$ is an \mds\ --- which
implies that $f_t$ is an \mds\ as discussed
at the beginning of this section. This is especially appealing in our
framework, since the benchmark can be theoretically motivated so the
\mds\ null would be a test of rationality. For example, \citet{GoW:08}
test whether excess returns for the S\&P 500 are predictable
out-of-sample, and any deviation of $\ep_{t+1}$ from an \mds\ is potentially
interesting. But the \mds\ null hypothesis only affects the estimator
of $\sigma^2$ (see Lemma~\ref{lem:2}); Theorem~\ref{res:1} continues
to hold under any \dgp\ that satisfies Assumptions~\ref{a1}~--~\ref{a4}.

In other settings, a researcher may want to test the weaker hypothesis
that $\E \fb^* = 0$ but the benchmark may be misspecified. Our
statistic can then be interpreted as an encompassing test as in
\citet{HLN:98}, and would test whether the alternative model contains
additional information that could make the benchmark model more
accurate. This interpretation can be motivated by the combination forecasting
model
\begin{equation*}
  \yh_{\mathit{avg},t+1} = (1 - w) x_t'\btrue + w \yh_{t+1}
\end{equation*}
which can be rewritten in terms of forecast errors as
\[
y_{t+1} - \yh_{\mathit{avg},t+1} = \ep_{t+1} + w (x_t'\btrue - \yh_{t+1}).
\]
The value
\[
w = \frac{\E \ep_{t+1} (\yh_{t+1} - x_t'\btrue)}{\E (x_t'\btrue - \yh_{t+1})^2}
\]
minimizes the \mse\ of the combination forecast, so the combination
model will have smaller \mse\ than the benchmark model, implying that
the alternative uses information not in the benchmark, unless
$\ep_{t+1}$ and $\yh_{t+1} - x_t'\btrue$ are uncorrelated. This
correlation is exactly the quantity measured by our statistic.

The final result puts together Theorem~\ref{res:1} and
Lemma~\ref{lem:2} to produce our test statistics. The null hypothesis
under misspecification is written in terms of $\E \ep_{t+1} \yh_{t+1}$
and not $\E \ep_{t+1} (\yh_{t+1} - x_t'\btrue)$, since $\E \ep_{t+1}
x_t = 0$ by construction. This result is an immediate consequence of
the previous two results and its proof is omitted.

\phantomsection
\addcontentsline{toc}{subsubsection}{Theorem \ref{thm:3}}
\begin{thm}\label{thm:3}\input{mixedwindow_thm3}\end{thm}

The test statistic proposed in Theorem~\ref{thm:3} can be easily
extended in several ways. For longer-horizon forecasts (two or more
periods ahead), $\sigmah_1$ will remain consistent but $\sigmah_2$
will not --- the forecast errors for a correctly specified
$h$-step-ahead forecast have an MA($h-1$) dependence structure --- but
using a generalized $\sigmah_2$ that reflects this covariance
structure restores consistency. To test optimality under loss
functions other than squared-error, one can replace the forecast error
with the generalized forecast error \citep[see, for
example][]{PaT:07,PaT:07b} and replace the \ols\ estimator of $\beta$ with the
corresponding $M$-estimator. And the benchmark model can be replaced
in general with a nonlinear model that satisfies the assumptions of
\citet{Wes:96} or \citet{Mcc:00} by making the appropriate changes to
$f_t$ and $g_t$. (See \citealp{Wes:96}, and \citealp{Mcc:00},
for details.) The general approach of using a recursive window to
estimate the benchmark and a fixed-length rolling window to estimate
the alternative applies quite broadly.

\section{Monte Carlo Results}\label{sec:2}
This section presents Monte Carlo experiments demonstrating that
this paper's modified version of \citepos{ClW:07} statistic performs
similarly to their original test in the situations they study, but can
have substantially higher power when the \dgp\ has a structural
break.%
\footnote{All of these simulations were programmed in R
  \citep[version 2.14.0]{R} and use the \allcaps{MASS}
  \citep[7.3-22]{VeR:02} package.} %

The \dgp\ has three different parametrizations: one to study the
tests' size, one to study power under stationarity, and one to study
power if there is a single break in the relationship between the
target and predictors.  The \dgp\ is:
\begin{align*}
  y_{t+1} &= \gamma_{1t} + \gamma_{2t} x_{t} + \ep_{t+1} &
  \gamma_t &=
  \begin{cases}
    (0.5, 0)    & \text{size simulations} \\
    (0.5, 0.35) & \text{power (stable)} \\
    (-0.5, 0)    & t \leq \tfrac{T}{2} \quad \text{power (break)} \\
    (1, 0.35) & t > \tfrac{T}{2} \quad \text{power (break)}
  \end{cases}\\\nonumber
  x_{t+1} &= 0.15 + 0.95 x_{t} + u_{t+1} &
  (\ep_t, u_t)' &\sim iid\ N\Bigg(\begin{pmatrix} 0 \\ 0
  \end{pmatrix}
   , \begin{pmatrix} 18 & -
    0.5 \\ -0.5 & 0.025 \end{pmatrix}\Bigg)
  \\ R &= 120, 240 & P &= 120, 240, 360, 720.
\end{align*}
Both models are estimated by \ols. The benchmark model regresses $y_{t+1}$
on a constant, and the alternative regresses $y_{t+1}$ on a constant and
$x_t$.  \citet{ClW:07} argue that this \dgp\ mimics an asset
pricing application similar to \citepos{GoW:08} which we study in
Section~\ref{sec:3}.

For comparison, we study this paper's new statistic as well as
\poscw\ rolling-window and recursive-window test statistics.  Clark and West
only prove that their rolling-window statistic is asymptotically
normal, and only then if the benchmark model is not estimated, but
their recursive-window statistic is popular in practice and in
simulations tends to perform similarly to their rolling window test.
We use all three of these statistics to test the null that the
benchmark model's innovation is an \mds.%
\footnote{\citet{ClW:07}
  report the performance of the tests proposed by \citet{CCS:01} and
  \citet{ClM:05} as well, and of tests based on the naive Gaussian
  statistic.} %

\begin{table}[tb]
  \centering
  \input{tex/mc1}
  \caption{Size and power of the \oos\ tests in the simulations
    described by Section~\ref{sec:2}, at
    \testsize\% confidence.  These percentages are calculated from \totalsims\
    samples.  Pr[\allcaps{CW} roll.] shows the fraction of simulations for
    which Clark and West's (2007) rolling-window statistic rejects;
    Pr[\allcaps{CW} rec.] shows the fraction of simulations for which
    their recursive-window statistic rejects; and Pr[new] shows the fraction of
    simulations for which this paper's test rejects.}
\label{tab:mc1}
\end{table}

Table~\ref{tab:mc1} presents the simulation results.  For all of the
stable parameter values, the proposed new statistic has similar
rejection probability to \citepos{ClW:07}.  Both of Clark and West's
tests are generally slightly undersized relative to our new test,
which is itself slightly undersized: when $R$ is 120 and $P$ is 360
our test statistic has size 7.6\% and Clark and West's rolling and
recursive window tests have size 7.5\% and 6.2\% respectively, at a
nominal size of 10\%.  For the stable alternative, our new statistic
typically has slightly higher power than Clark and West's rolling
window and lower power than their recursive window.  For example, when
$R$ is 120 and $P$ is 720, the rolling-window test rejects at 66.8\%,
our statistic at 73.0\%, and the recursive window statistic at 82.3\%,
again for a nominal size of 10\%.  In general, the statistics perform
similarly under stability.

For the simulations with a single break, the new statistic has
considerably higher power than \poscw\ original tests across all of
the choices of $R$ and $P$; the rejection probability is more than
twice as large for most parametrizations.  When $R$ is 120 and $P$ is
360 with a nominal size of 10\%, for example, the new statistic
rejects at 96.4\% while the rolling and recursive window statistics
reject at 35.5\% and 32.9\% respectively.  Results for other choices
of nominal size and sample split give similar results.  So mixing
window strategies can give a large power advantage when testing for
time-varying predictability, and performs similarly to the original
test when testing for stable outperformance.

\section{Empirical Illustration}\label{sec:3}

This section demonstrates the use of our new statistic by revisiting
\citepos{GoW:08} study of excess stock returns.  Goyal and Welch argue
that many variables thought to predict excess returns (measured as the
difference between the yearly log return of the S\&P 500 index and the
T-bill interest rate) on the basis of in-sample evidence fail to do so
out-of-sample.  To show this, Goyal and Welch look at the forecasting
performance of models using a lag of the variable of interest, and
show that these models do not significantly outperform the excess
return's recursive sample mean.

Here, we conduct the same analysis, but using this paper's \mds\ test.
The benchmark model is the excess return's sample mean (as in the
original) and the alternative models are of the form
\begin{equation*}
  \mathit{excess~return}_{t+1} = \beta_{0} + \beta_{1}\
  \mathit{predictor}_{t} + \ep_{t+1},
\end{equation*}
where $\beta_{0}$ and
$\beta_{1}$ are estimated by \ols\ using a \windowlength-year window.
The predictors used are listed in the \emph{predictor} column of
Table~\ref{tab:em1}. \citep[See][for a detailed description of the
variables.]{GoW:08}  We also consider \citepos{CaT:08} proposed
correction to the models, that the forecasts be bounded below by zero
since negative forecasts are incredible, as well as two simple
combination forecasts, the mean and the median (over both the original
and the non-negative forecasts).  The data set is annual data
beginning in 1927 and ending in 2009, and the rolling window uses
\windowlength\ observations.%
\footnote{This statistical analysis was conducted in R \citep{R} and
  uses the MASS \citep[7.3-22]{VeR:02} package.} %

\begin{table}[tb!]
  \centering
  \empiricaltable
\caption{Results from \oos\ comparison of equity premium prediction
  models; the benchmark is the recursive sample mean of the equity
  premium and each alternative model is a constant and single lag of
  the variable listed in the \emph{predictor} column.  The dataset begins
  in 1927 and ends in 2009 and is annual data. The \emph{value} column
  lists the value of this paper's \oos\ statistic, the \emph{naive}
  column indicates whether the statistic is significant at standard
  critical values, and the \emph{corrected} column indicates significance
  using critical values that
  account for the number of models.  See Section~\ref{sec:3} for details.}
\label{tab:em1}
\end{table}

Table~\ref{tab:em1} presents the results for each model.  The column
\emph{value} gives the value of the test statistic for each model,
while \emph{naive} indicates whether the statistic is greater than the
standard 10\% critical value (\naivecriticalvalue). Three predictors
are significant at the naive critical values for both the original and
bounded forecasts: the dividend yield, long term interest rate, and
book to market ratio, and the median forecast is significant as well.
This could suggest that excess returns are not an \mds\ and that
information in these three variables is useful for predicting returns.

However, we know that this is an extremely optimistic assessment of
the models' performance. We are conducting \nmod\ simultaneous
hypothesis tests, so it is likely that some will reject by chance.
There are several approaches that could accommodate this multiplicity and a full
treatment is beyond the scope of this paper, however, it is
straightforward to use our results to
derive a valid critical value similar to~\citet{Whi:00}.

Let $\fb_i$ be the \oos\ statistic associated with the $i$th alternative
forecast, $\yh_{i,t+1}$. The arguments underlying our results apply essentially unchanged to
multivariate $f_t$, so the continuous mapping theorem implies that
\begin{equation*}
  \max_{i=1,\dots,\nmod} \sqrt{P} \fb_i/\sigmah_{2i} \to^d \max_{i=1,\dots,\nmod} W_i,
\end{equation*}
where $W \sim N(0, V)$ and $V$ is the $\nmod
\times \nmod$ correlation matrix with elements
\begin{equation*}
  V_{ij} = \lim \frac{\cov(\fb_i, \fb_j)}{\var(\fb_i)^{1/2} \var(\fb_j)^{1/2}}.
\end{equation*}

To estimate $V$, we use the correlation matrix associated with the
multivariate analogue of $\sigmah_2$,
\[
\oavg{t}\Big[(\fh_t - \fb)(\fh_t - \fb)' + (\fh_t - \fb)(\gh_t - \gb)'
+ (\gh_t - \gb)(\fh_t - \fb)' + 2 (\gh_t - \gb)(\gh_t - \gb)' \Big]
\]
where $\fh_t$ and $\gh_t$ are vectors with $i$th elements
\begin{gather*}
  \fh_{it} = (y_{t+1} - x_t'\bh_t)^2 - (y_{t+1} - \yh_{i,t+1})^2 + (x_t'\bh_t - \yh_{i,t+1})^2
  \intertext{and}
  \gh_{it} = 2\, \Bigg[\oavg{s} (x_s'\bh_s - \yh_{i,s+1}) x_s'\Bigg]\,
            \Bigg[\tfrac{1}{T-1} \sum_{s=1}^{T-1} x_s x_s'\Bigg]^{-1} x_t \eph_{t+1}
\end{gather*}
respectively. Call this estimate $\hat V$ and let $\hat c$ denote the
$0.90$ quantile of the distribution of $\max_i \hat W_i$, with
$\hat W \sim N(0, \hat V)$.  Then
\begin{equation*}
  \limsup_{T \to \infty} \Pr\Big[\max_{i=1,\dots,\nmod} \sqrt{P} \fb_i / \sigmah_{2i} > \hat c\Big] \leq 0.10,
\end{equation*}
under the null hypothesis that excess returns are an \mds\ with
respect to all of the information contained in the variables listed in
Table~\ref{tab:em1}, making $\hat c$ an asymptotically valid critical
value.%
\footnote{\citet{Han:05} makes the point that multiple one-sided
  comparisons can have poor power if irrelevant predictors are
  included in these tests and proposes a threshold for discarding very
  poor forecasts. His threshold is well below our worst performing
  model, so this issue is not a concern here.} %

We calculate $\hat c$ by generating \empiricaldraws\ draws from
$N(0,\hat V)$, giving a value of \empiricalcriticalvalue, and the
\emph{corrected} column of Table~\ref{tab:em1} denotes the models that
remain significant at 10\% with this critical value. Using this
critical value, none of the predictors are significant, which
gives additional support to \citepos{GoW:08} conclusion that excess
returns are unpredictable and also demonstrates the importance of
correcting for multiplicity in these studies.

\section{Discussion}\label{sec:4}
This paper presents an \oos\ test statistic similar to \poscw\ that is
asymptotically normal when comparing nested or non-nested models.
Normality is achieved by estimating the alternative model using a
fixed-length rolling window --- as do Clark and West --- but
estimating the benchmark model with a recursive window.  Simulations
indicate that the new statistic behaves similarly to Clark and West's
original test when the \dgp\ is stable but can have much higher power
when the \dgp\ has structural breaks. We also have presented an
empirical study of the equity premium that demonstrates how to use
these results with several alternative models.

\newpage
\phantomsection
\addcontentsline{toc}{section}{References}
\bibliography{texextra/references}

\newpage
\appendix
\section{Supplemental appendix (not for publication)}
\newcommand{\WesA}[1][]{\ocltb{t}
  (F_t^{#1} - F) B^{#1} H_t^{#1}}
\newcommand{\WesB}[1][]{\ocltb{t} F (B_t^{#1} -
  B^{#1}) H_t^{#1}}
\newcommand{\WesC}[1][]{\ocltb{t}
  (F_t^{#1} - F) (B_t^{#1} - B^{#1}) H_t^{#1}}

This appendix contains mathematical proofs and some supporting Lemmas
for the paper, ``An asymptotically normal out-of-sample test based on
mixed estimation windows'' \citep{Cal:15}. Define the following
additional terms:
\begin{equation*}
  F_t(\beta) = 2 (2 x_t'\beta - \yh_{t+1} - y_{t+1}) x_t',
\end{equation*}
$F_t = F_t(\btrue)$, $\Fh_t = F_t(\bh_t)$, $F = \E F_t$, $B = (\E x_t
x_t')^{-1}$, $B_t = (\sum_{s=1}^{t-1} x_s x_s' / (t-1))^{-1}$, and
$H_t = \sum_{s=1}^{t-1} x_s \ep_{s+1} / (t-1)$.
And let $\lVert \cdot \rVert$ denote the $L_2$ norm in $\Re^k$.
Note that Assumptions~\ref{a1} and~\ref{a3} imply that $f_t$, $g_t$,
and $F_t$ are all strong mixing of size $-r/(r-2)$ or uniform mixing
of size $-r/(2r-2)$ and are stationary with bounded $r$th moments.

\subsection{Proof of Theorem \ref{res:1}}
  Let $R'$ be a new sequence such that $R' \to \infty$ as $T \to \infty$
  and $R' = o(\sqrt{P})$, and then rewrite the centered \oos\ average as
  \begin{equation}\label{eq:6}
    \sqrt{P} (\fb - \E \fb^*)
    = \ocltb{t} (\fh_t - \E f_t)
      + \tfrac{1}{\sqrt{P}} \osumc{t} (\fh_t - \E f_t).
  \end{equation}
  Lemma~\ref{res:a1} ensures that the second summation is $o_p(1)$, so
  we can rewrite~\eqref{eq:6} as
  \begin{align*}
    \sqrt{P} (\fb - \E \fb^*)
    &= \ocltb{t} (f_t - \E f_t) + F B \ocltb{t} H_t \\
    & \quad + \WesA + \WesB \\ & \quad + \WesC + \oclt{t} w_t + o_p(1)
  \end{align*}
  where $w_t$ equals $2 (\bh_t - \btrue)' x_t x_t' (\bh_t - \btrue)$.
  Lemma~\ref{res:a4} shows that
  \begin{gather}
    \WesA \to^{p} 0 \label{eq:11} \\
    \WesB \to^{p} 0 \label{eq:12} \\
    \intertext{and}
    \WesC \to^{p} 0 \label{eq:13}
  \end{gather}
  and Lemma~\ref{res:a2} along with the \clt\ ensures that $\oclt{t}
  w_t = o_{p}(1)$. The proof that
  \begin{equation*}
    \ocltb{t} (f_t - \E f_t) + F B \ocltb{t} H_t \to N(0, \sigma^2).
  \end{equation*}
  follows the same argument as in \citet{Wes:96} and \citet{Mcc:00}.

\subsection{Proof of Lemma~\ref{lem:2}}

We will only prove $\sigmah_2 \to^p \sigma$. The result for
$\sigmah_1$ is essentially the same and uses \citepos{JoD:00} Theorem
2.1 for the \hac\ equivalent of Equations~\eqref{eq:1}--\eqref{eq:7}.

  First, we can rewrite the components of the variance estimator as
  \begin{align*}
    \sh_{21} &= \oavg{t} \Big[(f_t - \E f_t) + (\fh_t - f_t) - (\fb - \E f_t)\Big]^2 \\
    \sh_{22} &= \oavg{t} \Big[(f_t - \E f_t) + (\fh_t - f_t) - (\fb - \E f_t)\Big]
                        \Big[(g_t - \E g_t) + (\gh_t - g_t) - (\gb - \E g_t)\Big]
    \intertext{and}
    \sh_{23} &= \oavg{t} \Big[(g_t - \E g_t) + (\gh_t - g_t) - (\gb + \E g_t)\Big]^2
  \end{align*}
  so $\sigmah_2 \to^p \sigma$ as long as the following hold:
  $\fb - \E \fb^* \to^p 0$,
  $\gb - \E \gb^* \to^p 0$,
  \begin{gather}
    \oavg{t} (f_t - \E f_t)^2 \to^p \lim \var(\sqrt{P} \fb^*) \label{eq:1} \\
    \oavg{t} (g_t - \E g_t)^2 \to^p \lim \var(\sqrt{P} \gb^*) \label{eq:3} \\
    \oavg{t} (f_t - \E f_t) (g_t - \E g_t) \to^p \lim \cov(\sqrt{P} \fb^*, \sqrt{P} \gb^*) \label{eq:7} \\
    \oavg{t} (\fh_t - f_t)^2 \to^p 0, \label{eq:4}
    \intertext{and}
    \oavg{t} (\gh_t - g_t)^2 \to^p 0. \label{eq:5}
  \end{gather}
  The first two results are implied by the proof of
  Theorem~\ref{res:1} and~\eqref{eq:1}, \eqref{eq:3}, and~\eqref{eq:7}
  follow from the \lln, since each summand is an $L_1$-mixingale of
  size $-1$ \citep[see, for example][Theorem 17.5]{Dav:94}, so it suffices to
  prove~\eqref{eq:4} and~\eqref{eq:5}.

  As in the proof of Theorem~\ref{res:1}, let $R'$ be a new sequence such that $R' \to \infty$ as
  $T \to \infty$ and $R' = o(\sqrt{P})$.  Straightforward algebra reveals
  that~\eqref{eq:4} holds if
  \begin{gather}
    \oavg{t} ((\bh_t - \btrue)' x_t)^4 \to^p 0 \label{eq:10}
    \intertext{and}
    \oavg{t} (x_t'(\bh_t - \btrue))^2 (2 x_t'\btrue - y_{t+1} - \yh_{t+1})^2 \to^p 0.\label{eq:14}
  \end{gather}
  The \allcaps{LHS} of~\eqref{eq:10} is bounded by
  \begin{align*}
    \oavg{t} &\|\bh_t - \btrue\|^4 \|x_t\|^4\\
    &= \oavgc{t} \|\bh_t - \btrue\|^4 \|x_t\|^4 + \oavgb{t} \|\bh_t - \btrue\|^4 \|x_t\|^4 \\
    &\leq \omaxc{t} \|\bh_t - \btrue\|^4 \,  \oavgc{t} \|x_t\|^4 + \omaxb{t} \|\bh_t - \btrue\|^4 \,  \oavgb{t} \|x_t\|^4 \\
    &= O_p(R'/P) + o_p(1)
  \end{align*}
  by Lemma~\ref{res:a2} and the \lln.
  A similar argument holds for the second term:
  \begin{align*}
    \oavg{t} (x_t'(\bh_t - \btrue))^2 &(2 x_t'\btrue - y_{t+1} - \yh_{t+1})^2 \\
    &= \oavgc{t} (x_t'(\bh_t - \btrue))^2 (2 x_t'\btrue - y_{t+1} - \yh_{t+1})^2 \\
    &\quad+ \oavgb{t} \big(x_t'(\bh_t - \btrue)\big)^2 (2 x_t'\btrue - y_{t+1} - \yh_{t+1})^2 \\
    &\leq \omaxc{t} \|\bh_t - \btrue\|^2 \oavgc{t} \|x_t (2 x_t'\btrue - y_{t+1} - \yh_{t+1})\|^2 \\
    &\quad+ \omaxb{t} \|\bh_t - \btrue\|^2 \oavgb{t} \|x_t (2 x_t'\btrue - y_{t+1} - \yh_{t+1})\|^2 \\
    &= O_p(R'/P) + o_p(1)
  \end{align*}
  again by Lemma~\ref{res:a2} and the \lln. Both terms converge to
  zero in probability by construction. The proof of~\eqref{eq:5} is similar.

\subsection{Supporting results}

\phantomsection
\addcontentsline{toc}{subsubsection}{Lemma \ref{res:a1}}
\begin{alem}\label{res:a1}
  Suppose the conditions of Theorem~\ref{res:1} hold, and define $R'$
  to be a sequence that satisfies $R' \to \infty$ as $T \to \infty$
  and $R' = o(\sqrt{P})$. Then
  \begin{equation*}
    \ocltc{t} (\fh_t - \E f_t) \to^p 0.
  \end{equation*}
\end{alem}

\begin{proof}
  We can rewrite this summation as
  \begin{multline*}
    \ocltc{t} (\fh_t - \E f_t) = \ocltc{t} (f_t - \E f_t) + \\
    \ocltc{t} (4 x_t'\btrue - 2 y_{t+1} - 2 \yh_{i,t+1}) x_t'(\bh_t - \btrue)
    + \ocltc{t} (x_t'\bh_t - x_t'\btrue)^2.
  \end{multline*}
  Each of these individual summations can be shown to converge to
  zero in probability. First,
  \begin{equation*}
    \E \Big\lvert \ocltc{t} (f_t - \E f_t) \Big\rvert
    \leq \ocltc{t} \E\lvert f_t - \E f_t \rvert
    = O(R'/\sqrt{P}).
  \end{equation*}
  Also,
  \begin{align*}
    \Big\lvert \ocltc{t} & (4 x_t'\btrue - 2 y_{t+1} - 2 \yh_{i,t+1}) x_t'(\bh_t - \btrue) \Big\rvert \\
    &\leq \ocltc{t} \big\lVert (4 x_t'\btrue - 2 y_{t+1} - 2 \yh_{i,t+1}) x_t \big\rVert
    \omaxc{t} \lVert  \bh_t - \btrue \rVert \\
    & = O_p(R'/\sqrt{P})
  \end{align*}
  and
  \begin{equation*}
    \Big\lvert \ocltc{t} (x_t'\bh_t - x_t'\btrue)^2 \Big\rvert
    \leq \ocltc{t} \lVert x_t \rVert^2 \omaxc{t} \lVert \bh_t - \btrue \rVert^2
    = O_p(R'/\sqrt{P}).
  \end{equation*}
  by Lemma~\ref{res:a2} and the \lln. Since $R'/\sqrt{P} \to 0$ by
  construction, this completes the proof.
\end{proof}

\phantomsection
\addcontentsline{toc}{subsubsection}{Lemma \ref{res:a2}}
\begin{alem}\label{res:a2}
  Suppose $a \in [0,1/2)$ and Assumptions~\ref{a1}~--~\ref{a4}
  hold, and let $R'$ be a sequence such that $R' \to \infty$ as $T \to
  \infty$ and $R' = o(\sqrt{P})$. Then
  \begin{enumerate}
  \item $\omaxb{t} | (t-1)^a H_t | \to^p 0$,
  \item $\omaxc{t} | (t-1)^a H_t | = O_p(1)$,
  \item $\omaxb{t} | B_t - B | \to^p 0$,
  \item $\omaxc{t} | B_t - B | = O_p(1)$,
  \item $\omaxb{t} | (t-1)^a(\bh_t - \btrue) | \to^{p} 0$, and
  \item $\omaxc{t} | (t-1)^a(\bh_t - \btrue) | = O_p(1)$,
  \end{enumerate}
  where the absolute value is taken as the largest of the
  element-by-element absolute values.
\end{alem}

\noindent%
To streamline the
presentation, we'll assume in these proofs that $x_t$ is a scalar.
\begin{proof}
  We will prove each part in order.
  \begin{enumerate}
  \item Our assumptions ensure that $x_t \ep_{t+1}$ is $L_2$-mixingale
    of size $-1/2$ \citep[see Theorem 17.5 of][]{Dav:94}; let $c_t$
    and $\zeta_k$ denote its mixingale
    constants and coefficients. Note that, for any $b$, $t^{b} x_t \ep_{t+1}$ is
    also an $L_2$-mixingale array with constants $t^{b} c_s$ and
    coefficients $\zeta_k$, since
    \begin{align*}
      \| \E_{t-k} t^{b} x_t \ep_{t+1} \| &= t^{b} \| \E_{t-k} x_t \ep_{t+1} \| \\
      &\leq (t^{b} c_t) \zeta_k
    \end{align*}
    and
    \begin{align*}
      \| t^{b} x_t \ep_{t+1} - t^{b} \E_{t+k} x_t \ep_{t+1} \| &= t^{b} \|  x_t \ep_{t+1} - \E_{t+k} x_t \ep_{t+1} \| \\
      &\leq (t^{b} c_t) \zeta_{k+1}.
    \end{align*}

    Let $\delta$ be a positive number less than $1/2 - \alpha$, so
    \begin{align*}
      \E\Bigg[\omaxb{t} \Big|(t-1)^{a-1} & \sum_{s=1}^{t-1} x_s \ep_{s+1} \Big|^2\Bigg] \\
      &\leq (R'-1)^{-2\delta} \E\Bigg[\omaxb{t} \Big| \sum_{s=1}^{t-1} x_s \ep_{s+1} (s-1)^{a-1+\delta} \Big|^2\Bigg] \\
      &\leq (R'-1)^{-2\delta} O(1) \sum_{s=1}^{T-1} (s-1)^{2(a - 1 + \delta)}
    \end{align*}
    where the second inequality follows from \citepos{Mcl:75} maximal
    inequality (also available as \citealp{Dav:94}, Theorem 16.9 and
    Corollary 16.10). The summation converges to a constant and
    $(R'-1)^{-2\delta} \to 0$ as $T \to \infty$, completing the proof.

  \item Now $t^{a-1} x_t \ep_{t+1}$ is an $L_2$-mixingale of size
    $-1/2$ and we can again use \citepos{Mcl:75} maximal inequality to get
    \begin{align*}
      \E \Big\lvert \omaxc{t} \Big((t-1)^{a - 1} \sum_{s=1}^{t-1} x_s \ep_{s+1} \Big)^2 \Big\rvert
      &\leq \E \Big\lvert \omax{t} \Big(\sum_{s=1}^{t-1} s^{a-1} x_s \ep_{s+1} \Big)^2 \Big\rvert \\
      &= O(1) \sum_{s=1}^{R'-1} s^{2a - 2}
    \end{align*}
    which converges to a finite limit.
  \item The same argument used in Part 1 implies that $\omaxb{t} |
    B_t^{-1} - B^{-1}| \to^p 0$. Since matrix inversion is continuous,
    the result follows.
  \item Holds by Assumptions~\ref{a1} and~\ref{a3}.
  \item We have
    \begin{align*}
      \omaxb{t} | (t-1)^a (\hat{\beta}_t - \btrue) |
      &\leq \omaxb{t} |\hat{B}_t - B|
      \omaxb{t} \Big|(t-1)^{a-1} \sum_{s=1}^{t-1} x_s \ep_{s+1} \Big| \\
      &\quad + \omaxb{t} \Big| B (t-1)^{a-1} \sum_{s=1}^{t-1} x_s \ep_{s+1} \Big|
    \end{align*}
    and both terms converge to zero in probability by Parts 1 and 3.
  \item Similar to the previous argument, we have
    \begin{align*}
      \omaxc{t} | (t-1)^a (\hat{\beta}_t - \btrue) |
      & \leq \omaxc{t} | \hat{B}_t - B | \omaxc{t} \Big|(t-1)^{a-1} \sum_{s=1}^{t-1} x_s \ep_{s+1} \Big| \\
      &\quad + \omaxc{t} \Big| B (t-1)^{a-1} \sum_{s=1}^{t-1} x_s \ep_{s+1} \Big|.
    \end{align*}
    Both terms are $O_p(1)$ by Parts 2 and 4. \qedhere
  \end{enumerate}
\end{proof}

\phantomsection
\addcontentsline{toc}{subsubsection}{Lemma \ref{res:a4}}
\begin{alem}\label{res:a4}
  Under the conditions of Theorem~\ref{res:1}, Equations
  \eqref{eq:11}--\eqref{eq:13} hold.
\end{alem}

\begin{proof}[Proof of~\eqref{eq:11}]
We can write
\begin{equation*}
  \Big\lvert \WesA \Big\rvert \leq
  \Big\lVert \ocltb{t} (F_t - F) B \Big\rVert \;
  \omaxb{t} \lVert H_t \rVert.
\end{equation*}
From Lemma~\ref{res:a2}, $\omaxb{t} \lVert H_t \rVert \to^p
0$. \citepos{Jon:97} \clt\ implies that $\ocltb{t} (F_t - F) =
O_p(1)$, establishing~\eqref{eq:11}.
\end{proof}

\begin{proof}[Proof of~\eqref{eq:12}]
TBD
\end{proof}

\begin{proof}[Proof of~\eqref{eq:13}]
TBD
\end{proof}

\setcounter{secnumdepth}{1}
\include{CHANGELOG}

\end{document}

% LocalWords:  ClW JEL ISI Google GiW Mcc ClM CoS CCS StW IMA GiR WeM fh X'X hh
% LocalWords:  PaT AllRefs isi ima uc sv PeT GoW lm il GoK RoW Econometrica PoR
% LocalWords:  Finan StepM studentizing studentization Whi HuW RSW recentering
% LocalWords:  DiM LiS Kun McCracken lt filtrations GoJR JoD McCracken's Econom
% LocalWords:  Corradi unstudentized studentized GoJ gcalhoun HLN li PoW
% LocalWords:  Econometricians reestimate PPW resample miscentered AnG eq Helle
% LocalWords:  Bunzel Yu Hsu Pincheira HHK th AtO ik DoH Wolak's Wol stepdown
% LocalWords:  Hsu's iq mk Ames Amit Goyal rfs Ivo Welch Wes covariance MeR McW
% LocalWords:  familywise prespecified CoD Gia pointwise misspecified InK MeP
% LocalWords:  VeR xtable Dah dbframe oos parametrizations iid dgp return's CaT
% LocalWords:  outperformance Hmisc Har nondifferentiable bT jt Meng
% LocalWords:  stationarity mixingale mixingales texextra Timmermann

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
