\documentclass[12pt,fleqn]{article}

\usepackage{amsfonts}
\usepackage{amsmath,amsthm,amssymb,graphicx,setspace,url,booktabs,tabularx,enumerate,slantsc}
\usepackage[T1]{fontenc}
\usepackage[sort,round,comma]{natbib}
\usepackage[margin=1.25in]{geometry}
\usepackage[small]{caption}
\usepackage[charter]{mathdesign}
\urlstyle{same}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\bibliographystyle{abbrvnat}
\newcommand\citepos[2][]{\citeauthor{#2}'s \citeyearpar[#1]{#2}}
\newcommand\poscw{\citeauthor{ClW:06}'s \citeyearpar{ClW:06,ClW:07}}
\newcommand\citen[1]{\citeauthor{#1}, \citeyear{#1}}
\frenchspacing
\input{tex/mcDef}
\input{tex/ap}

% These commands are generated when the monte carlo and applied
% sections are run; I'm giving them definitions now so that LaTeX will
% run.
\providecommand\bootsize{[missing]}
\providecommand\empiricalcriticalvalue{[missing]}
\providecommand\nboot{[missing]}
\providecommand\testsize{[missing]}
\providecommand\totalsims{[missing]}
\providecommand\windowlength{[missing]}
\providecommand\empiricaltable{[missing]}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{claim}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lema}{Lemma}[section]
\newtheorem{alg}{Algorithm}
\newtheorem{asmp}{Assumption}

\theoremstyle{definition}

\newtheorem{example}{Example}
\newtheorem{defn}{Definition}
\newtheorem{rem}{Remark}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
%\DeclareMathOperator{\vec}{vec}
\DeclareMathOperator{\vech}{vech}

\DeclareMathOperator{\pr}{Pr}

\newcommand{\Bh}{\hat{B}}
\newcommand{\btrue}[1][]{\if#1*\hat\beta_{T+1}\else\beta_0\fi}
\newcommand{\bh}{\hat{\beta}}
\newcommand{\bt}{\tilde{\beta}}
\newcommand{\ep}{\varepsilon}
\newcommand{\eph}{\hat{\varepsilon}}
\newcommand{\Fh}{\hat{F}}
\newcommand{\Fs}{\mathcal{F}}
\newcommand{\fb}{\bar{f}}
\newcommand{\fh}{\hat{f}}
\newcommand{\gb}{\bar{g}}
\newcommand{\gh}{\hat{g}}
\newcommand{\Sh}{\hat{S}}
\newcommand{\Sigmah}{\hat\Sigma}
\newcommand{\sh}{\hat\sigma}
\newcommand{\yh}{\hat{y}}

\newcommand{\X}{\ensuremath{\mathrm{X}}}
\newcommand{\R}{\ensuremath{\mathrm{R}}}
\newcommand{\p}{\ensuremath{\mathrm{P}}}

\newcommand{\osum}[1]{\sum_{#1=R+1}^T}
\newcommand{\oavg}[1]{\tfrac{1}{P} \osum{#1}}
\newcommand{\oclt}[1]{\tfrac{1}{\sqrt{P}} \osum{#1}}

\newcommand{\aic}{AIC}
\newcommand{\bic}{BIC}
\newcommand{\brc}{BRC}
\newcommand{\cdf}{CDF}
\newcommand{\clt}{CLT}
\newcommand{\dd}[1]{\frac{\partial}{\partial #1}}
\newcommand{\dgp}{DGP}
\newcommand{\fclt}{FCLT}
\newcommand{\fwe}{FWE}
\newcommand{\gdp}{GDP}
\newcommand{\hac}{HAC}
\newcommand{\lln}{LLN}
\newcommand{\ma}{MA}
\newcommand{\mds}{MDS}
\newcommand{\ned}{NED}
\newcommand{\ols}{OLS}
\newcommand{\oos}{OOS}
\newcommand{\sfwe}{SFWE}
\newcommand{\spa}{SPA}
\newcommand{\wfwe}{WFWE}

\renewcommand{\Re}{\ensuremath{\mathbb{R}}}

\renewcommand{\topfraction}{.85}
\renewcommand{\bottomfraction}{.7}
\renewcommand{\textfraction}{.15}
\renewcommand{\floatpagefraction}{.66}
\renewcommand{\dbltopfraction}{.66}
\renewcommand{\dblfloatpagefraction}{.66}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\setcounter{totalnumber}{20}
\setcounter{dbltopnumber}{9}

\begin{document}

\author{Gray Calhoun\thanks{ Economics Department; Iowa State
    University; Ames, IA 50011.  Telephone: (515) 294-6271.  Email:
    \guillemotleft \protect\url{gcalhoun@iastate.edu}\guillemotright,
    web: \guillemotleft \protect\url{http://gray.clhn.co}\guillemotright.
    I'd like to
    thank Helle Bunzel, Todd Clark, Graham Elliott, Yu-Chin Hsu,
    Michael McCracken, Pablo Pincheira, Allan Timmermann, Stephane
    Meng-Feng Yen and participants at the 2011 Midwest Econometrics
    Group meeting and the 2013 NBER-NSF Time Series conference
    for helpful comments and discussions.  I'd also like to thank Amit
    Goyal for providing computer code and data for his 2008
    RFS paper with Ivo Welch \citep{GoW:08}.}\\%
  Iowa State University}

\title{An asymptotically normal out-of-sample
  test of equal predictive accuracy for nested models} 

% \date{March 14, 2013}

\maketitle

\begin{abstract} 
  \noindent This paper develops a modification of \citepos[\textit{J.
    Econom.}]{ClW:07} adjusted out-of-sample $t$-test. We propose
  using a recursive window to estimate the benchmark model but a
  fixed-length rolling window to estimate the alternative. The
  resulting statistic is asymptotically normal even when the models
  are nested.  Moreover, the alternative model can be estimated using
  common model selection methods, such as the \aic\ or \bic, without
  affecting the asymptotic distribution of this test statistic.  The
  paper also presents Monte Carlo evidence that this statistic has
  much higher power than existing out-of-sample statistics in a common
  use-case for these tests: when the DGP is subject to instability.
  This procedure is then used to analyze
  \citepos[\textit{Rev. Finan. Stud.}]{GoW:08} excess returns dataset
  and supports their finding that the equity premium is unpredictable
  out-of-sample.

\strut

\noindent Keywords: Forecast Evaluation, Martingale Difference
Sequence, Model Selection

\strut

\noindent JEL Classification Numbers: C22, C53

\end{abstract}

\newpage 

\section{Introduction} This paper proposes an out-of-sample (\oos)
test statistic that is asymptotically normal with mean zero even when
the models studied are nested.
\oos\ tests are commonly used in International Macroeconomics,
Macroeconomics, and Finance (see, for example, \citealt{MeR:83};
\citealt{StW:03}; and \citealt{GoW:08}) and there is a substantial
literature developing the theoretical properties of these statistics,
beginning primarily with \citet{DiM:95} and
\citet{Wes:96}.%
\footnote{Other papers in this literature include
  \citet{WeM:98}, \citet{Mcc:98,Mcc:00},
  \citet{ClM:01,ClM:05-2,ClM:05,ClM:11b,ClM:12,ClM:12b},
  \citet{CCS:01}, \citet{CoS:02,CoS:04,CoS:07}, \citet{Whi:00},
  \citet{InK:04,InK:06}, \citet{Han:05}, \citet{Ros:05},
  \citet{ClW:06,ClW:07}, \citet{Ana:07}, \citet{GiR:09,GiR:10},
  \citet{HuW:10}, \citet{HLN:11}, \cite{InR:11}, \cite{Pin:11},
  \cite{RoS:11,RoS:11b}, and \citet{Cal:11}, among others.  For recent
  reviews of this literature and additional references, see
  \citet{McW:02}, \citet{CoS:06}, \citet{Wes:06}, \citet{ClM:11c},
  \citet{CoD:11}, and \citet{Gia:11}} %
In a pair of papers,
\citet{ClW:06,ClW:07} develop an \oos\ test of the null hypothesis
that a small benchmark model is correctly specified.  Their test
compares the forecasting performance of a pair of nested models, and
the null hypothesis is that the innovations in the smaller model form
a Martingale Difference Sequence (\mds).  This test procedure is popular, and
one assumes that this is due in part to the statistic's convenience,
the statistic is approximately normal after adjusting for the
estimation error of the larger model.  Normality comes from a
fixed-length rolling window, as in \citet{GiW:06}, and the adjustment
centers the statistic appropriately.  This statistic is especially
convenient because other \oos\ tests for similar hypotheses
(\citealt{CCS:01}; \citealt{ClM:01,ClM:05}; \citealt{CoS:02,CoS:04};
and \citealt{Mcc:07}; among others) have a nonstandard limit
distribution and place restrictions on the models under consideration,
while other asymptotically normal statistics test a different null
hypothesis \citep{GiW:06} or place assumptions on the models and \dgp\
that are often violated in empirical work (\citealt{DiM:95};
\citealt{Wes:96}; \citealt{WeM:98};
\citealt{Mcc:00}).%
\footnote{\citet{DiM:95} assume that the models are
  not estimated. \citet{Wes:96}, \citet{WeM:98}, and \citet{Mcc:00}
  implicitly assume that the models do not converge to the same limit,
  which rules out nesting.} %

However, Clark and West's statistic is only ``approximately normal''
in an informal sense.  Clark and West present Monte Carlo evidence of
the statistic's distribution, but only prove that the statistic is
asymptotically normal with mean zero when the benchmark model is a
random walk \citep{ClW:06}. Estimating the parameters of the smaller
model invalidates their proof.

In this paper, I show that a modified version of their statistic is
asymptotically normal even when the smaller model is estimated.  To
achieve normality, the pseudotrue benchmark model must be estimated
consistently, but the larger alternative model must continue to be
estimated inconsistently so that the test statistic is not degenerate
when the models are nested. We can meet both needs by
using different window strategies for each model: the benchmark model
is estimated using a recursive window and the alternative with a
fixed-length rolling window.

Mixing window strategies is uncommon but needn't be. In most
applications, the null hypothesis imposes stability as well as equal
accuracy between the two models.  The benchmark model rarely allows
for breaks, parameter drift, or other forms of
instability,%
\footnote{Exceptions are \citepos{StW:07}
  IMA(1,1) and UC-SV models of inflation.} %
but the
researcher is typically concerned about instability.  Indeed, concern
about instability is often given as a reason for doing an \oos\
analysis, especially with a short rolling window.%
\footnote{This
  motivation is discussed by \citet{StW:03}, \citet{PeT:05,PeT:07},
  \cite{GiW:06}, \citet{GoW:08}, \citet{ClM:09c}, and
  \cite{GiR:09,GiR:10}, among others.} %
A researcher could impose
stability on both models by using a recursive window or relax
stability for both by using a rolling window; either approach should
not affect the test's size, but may affect power.  But the researcher
could instead impose stability on the benchmark and relax it for the
alternative by using a recursive window for the benchmark and a
rolling window for the alternative model.  This approach could have a
power advantage and is similar in spirit to using a Likelihood Ratio
Test instead of an LM or Wald test, which depend on just the
restricted or unrestricted model respectively.

This paper's statistic has a substantial advantage over existing \oos\
tests for nested models: the alternative can be essentially arbitrary
as long as high level moment conditions hold.  In particular,
researchers can use model selection techniques like the \aic\ or \bic\
to determine the number of lags to include, the particular exogenous
variables to include, etc.  Other methods that test a similar
hypothesis are unable to handle these models \citep[except][which does
not allow the benchmark to be estimated]{ClW:06}; \citet{GiW:06} are
able to handle such models for both the alternative and the benchmark
but, as mentioned earlier, they test a different aspect of forecasting
performance.

This paper focuses on nested models, as they have received the most
attention in the empirical and theoretical literature, but the
statistic can be used with non-nested models as well.  This generality
is useful, since \citepos{Wes:96} results do not apply to non-nested
models if they both encompass the true \dgp,%
\footnote{\citet{ClM:11b}
  call this scenario, ``overlapping models.''} %
which is allowable
under the null: in the limit, both models will converge to the \dgp\
and give identical forecasts.  Consequently, the naive \oos\ $t$-test
is invalid, even after correcting the standard error if necessary to
reflect parameter uncertainty.  \citet{ClM:11b} show that the fixed
window \oos\ $t$-test remains normal for these models but the
recursive and rolling windows (with the window size increasing to
$\infty$) do not, and provide a procedure for pointwise (but not
uniformly) valid tests for the recursive and rolling windows and
uniformly valid tests for the fixed window.  The test proposed in this
paper is uniformly valid and places fewer assumptions on the models
under study and the true \dgp.

The next section presents our new statistics and Section~\ref{sec:2} presents
simulations that compare our test to \poscw\ original
statistics.  Section~\ref{sec:3} demonstrates the use of our statistic
by reanalyzing \citepos{GoW:08} study of excess return
predictability. Section~\ref{sec:4} concludes.

\section{Theoretical results supporting the asympotically normal OOS statistic}
\label{sec:1}

This section presents the new \oos\ statistic; first we give an
informal motivation of the statistc, then section~\ref{sec:1a}
presents the paper's key assumptions and section~\ref{sec:1b} presents
our formal theoretical results.

Suppose for now that a researcher is interested in
predicting the target variable $y_{t+1}$ with a vector of regressors
$x_t$; also let $v_t$ be another random process that may contain information about
$y_{t+1}$ and suppose that
$(y_t, x_t, v_t)$ is stationary and weakly dependent
In addition,
let $\btrue = (\E x_t x_t')^{-1} \E x_t y_{t+1}$ be the pseudotrue
coefficient of the regression of $y_{t+1}$ on $x_t$ and define
$\ep_{t+1} = y_{t+1} - x_t'\btrue$.  If this linear model is
correctly specified, then $\ep_{t+1}$ is an \mds\ with respect
to the information set
\begin{equation*}
\mathcal{F}_t \equiv \sigma((x_t, v_t, y_t), (x_{t-1}, v_{t-1},
y_{t-1}),\dots),
\end{equation*}
and we can see immediately that
\begin{equation}
  \label{eq:4}
  \oclt{t} \ep_{t+1} (v_t - x_t'\btrue)
\end{equation}
obeys an \mds\ \clt%
\footnote{This claim assumes that the asymptotic variance of the
  sample average is uniformly positive, a requirement that we will
  address in Section~\ref{sec:1b}.} %
and is asymptotically normal as $P \to \infty$, with $R$ an
arbitrary starting value%
\footnote{It will be clear momentarily why the
  summation begins at $R+1$ instead of $1$.} %
and $P = T - R$.%
\footnote{Section~\ref{sec:1b} discusses conditions under which the
  \mds\ condition holds. We will also present results that allow
  $\ep_{t+1}$ to be serially correlated.} %
Straightforward algebra \citep{ClW:07} shows that
\begin{equation}
  \label{eq:5}
  \tfrac{1}{\sqrt{P}} \osum{t} \ep_{t+1} (v_t -
  x_t'\btrue) = \tfrac{1}{2 \sqrt{P}} \osum{t} \Big[(y_{t+1} -
  x_t\btrue)^2 - (y_{t+1} - v_t)^2 + (x_t'\btrue - v_t)^2 \Big].
\end{equation}

\citet{ClW:06,ClW:07} base their \oos\ statistics on the RHS of
Equation~\eqref{eq:5}, but using a second forecast of $y_{t+1}$ as
$v_t$. (Call it $\yh_{t+1}$.) They use a rolling window of length
$R$ to estimate $\yh_{t+1}$;%
\footnote{Making $\yh_{t+1}$ a function of $y_t, x_{t-1}, z_{t-1}
\dots, y_{t-R+1}, x_{t-R}$ and $z_{t-R}$, where $z_t$ is another weakly
dependent random process} %
$R$ is kept finite as $T \to \infty$, so
$\yh_{t+1}$ inherits the weak dependence properties of the
variables used to estimate it. Using a finite window length $R$ avoids
the degeneracy that often arises in OOS test statistics (see
\citealp{ClM:01}, and \citealp{Mcc:07}), so the conditional variance
of the OOS average remains positive and the average obeys a \clt, an
approach introduced by \citet{GiW:06}.

\citet{ClW:06,ClW:07} develop a test of whether $\ep_{t+1}$ is
an \mds. In the 2006 paper, Clark and West assume that the
coefficients $\btrue$ are zero under the null, making
$\ep_{t+1}$ observed directly. This restriction is relaxed in
the 2007 paper where $\btrue$ is unknown and estimated with length-$R$
rolling window. Now the estimated linear model's prediction errors,
$\eph_{t+1}$, replace $\ep_{t+1}$ in the \oos\
test statistic. Unfortunately, $\eph_{t+1}$ is not an
\mds\ even when $\ep_{t+1}$ is, so the statistic is no longer
asymptotically mean-zero normal, even though this approximation
performs well in simulations.

This paper proposes using the same basic \oos\ statistic, 
but using a recursive window to estimate $\btrue$ and produce
$\eph_{t+1}$; so
\begin{equation}
  \label{eq:8}
  \bh_t = \Big(\sum_{s=2}^{t} x_{t-1} x_{t-1}'\Big)^{-1}
  \sum_{s=2}^t x_{t-1} y_t.
\end{equation}
\citepos{Wes:96} Theorem 4.1 implies that
\begin{equation*}
  \oclt{t} \Big[(y_{t+1} -
  x_t\bh_t)^2 - (y_{t+1} - v_t)^2 + (x_t'\bh_t - v_t)^2 \Big]
\end{equation*}
is asymptotically normal with mean zero under the null for fairly
arbitrary processes $v_t$, as long as $v_t$ is weakly dependent and
the \oos\ statistic has uniformly positive variance.  Just as in
\citet{ClW:06,ClW:07}, these conditions are ensured if $v_t$ is
another forecast of $y_{t+1}$ based on a fixed-length rolling window.

This overview has presented an especially simple version of the result
to make the intuition as clear as possible. The next section lists the
specific assumptions that need to hold.

\subsection{Assumptions}
\label{sec:1a}

The assumptions required are essentially the same in existing papers
(e.g. \citealp{Wes:96}; \citealp{WeM:98}; \citealp{Mcc:00};
\citealp{GiW:06}; and \citealp{ClW:06,ClW:07}).

\begin{asmp}\label{a1}%
  The benchmark forecast $\yh_{0,t+1}$, is estimated using \ols\
  with a recursive window: $\yh_{0,t+1} = x_t'\bh_t$ for
  some vector of predictors $x_t$ with $\bh_t$ given by
  Equation~\eqref{eq:8}.  Also define $\btrue = (\E x_{t-1}
  x_{t-1}')^{-1} \E x_{t-1} y_t$ and assume that $\btrue$ does not
  depend on~$t$.

  The alternative forecast, $\yh_{1t}$, is estimated using a
  rolling window of fixed length $R$ (which is less than $T$), so
  $\yh_{1,t+1} = \psi(y_t,z_t,\dots,y_{t-R+1}, z_{t-R+1})$ where
  $\psi$ is a known function and $z_t$ is a sequence of predictors
  that may include $x_t$.
\end{asmp}

\begin{asmp}\label{a3}%
  The series $y_t$, $\yh_{1t}$, and $x_t$ have uniformly bounded
  $2 r$ moments for some $r > 2$, and $\yh_t y_t$ $\yh_t
  x_{t-1}$, $y_t x_{t-1}$, and $x_{t-1}, x_{t-1}'$ are $L_2$-\ned\ of
  size $-\frac12$ on a strong mixing series of size $-\frac{r}{r-2}$
  for $r>2$ or a uniform mixing series of size $-\frac{r}{2r-2}$.
\end{asmp}

\begin{asmp}\label{a4}%
  Define \[f_t(\beta) = (y_{t+1} - x_t'\beta)^2 - (y_{t+1} -
  \yh_{1,t+1})^2 + (x_t'\beta - \yh_{1,t+1})^2,\] $f_t =
  f_t(\btrue)$, $\fh_t = f_t(\bh_t)$, $\fb(\beta) =
  \oavg{t} f_t(\beta)$, and $\fb = \tfrac1P \sum_{t=R+1}^{T} \fh_t$
  with $P = T - R$; $\fb(\btrue)$ has uniformly positive and
  finite long run variance.
\end{asmp}

\subsection{Theoretical results}
\label{sec:1b}

\begin{thm}\label{res:1}
  Suppose that we have two models $\yh_{0t}$ and $\yh_{1t}$ to
  forecast the variable $y_t$, and have observations for
  $t=1,\dots,T+1$. Suppose that Assumptions~\ref{a1} -- \ref{a4} hold.
  Under the null hypothesis that $y_t - \yh_{0t}(\btrue)$ is a
  martingale difference sequence with respect to the filtration
  $\mathcal{F}_t = \sigma((y_t, z_{t}), (y_{t-1}, z_{t-1}),\dots)$,
  $\tfrac{\sqrt{P}}{\sh} \fb \to^d N(0,1)$, where
  \begin{align*}
    \sh^2 &= \Sh_{ff} + 2 (\Sh_{fg} + \Sh_{gg}), &
    \Sh_{ff} &= \oavg{t} (\fh_t - \fb)^2, \\
    \Sh_{fg} &= \oavg{t} (\fh_t - \fb)(\gh_t - \gb)', &
    \Sh_{gg} &= \oavg{t} (\gh_t - \gb)(\gh_t - \gb)',
  \end{align*}
  $\gb = \oavg{t}
  \gh_t$, $\X' = [x_1,\dots,x_T]$,
  \begin{equation*}
    g_t(\beta) =
    \Big\{\tfrac{2}{P}\sum_{s=R}^T x_s (x_s'\beta - \yh_{1,s+1}) \Big\}'
    \big(\tfrac1T \X'\X \big)^{-1} x_t(y_{t+1} - x_t'\beta),
  \end{equation*}
  and $\gh_t = g_t(\bh_t)$.
\end{thm}

The following remarks are relevant to Theorem~\ref{res:1}:

\begin{rem}
  Forecasters are usually interested in the one-sided alternative that
  $\E f_t > 0$; i.e. that the alternative model is expected
  to forecast better than the benchmark.
\end{rem}

\begin{rem}
  These results are presented for one-period-ahead forecasting for
  simplicity.  They can be extended to forecasting at a longer horizon
  by appropriately modifying the variance-covariance matrix to account
  for the correlation structure of the forecast errors. (i.e. for
  $\tau$-step-ahead forecasts the errors will be an \ma($\tau-1$) process).
\end{rem}

\begin{rem}
  The requirement that the asymptotic variance of $\fb(\btrue)$ is
  uniformly positive is much less restrictive than in \cite{Wes:96}.
  As in \cite{GiW:06} and \citet{ClW:06,ClW:07}, the assumption only
  serves to rule out pathological cases---for example, letting both
  the benchmark and the alternative model be perfectly correlated white noise. In
  \citet{Wes:96}, this assumption is a restriction on the \dgp\ as
  well as the forecasting models, but in this paper it is a
  restriction only on the models.
\end{rem}

Define $\X' = [x_1,\dots,x_T]$

\begin{align*}
  \Fh &= \Big\{\tfrac{2}{P}\sum_{s=R}^T x_s (x_s'\beta - \yh_{1,s+1}) \Big\}' &
  \Bh &= \big(\tfrac1T \X'\X \big)^{-1}
\end{align*}

Define the variables
$g_t(\beta) = \Fh \Bh x_t(y_{t+1} - x_t'\beta)$,
$\gh_t = g_t(\bh_t)$, and $\gb = \oavg{t} \gh_t$.

\newcommand{\K}[1]{K(\tfrac{#1}{w})}

Define the general covariance estimator:
\begin{align*}
  \Sigmah_1 &= \Sh_{21} + \Sh_{22} + \Sh_{22}' + 2 \, \Sh_{23}, &
  \Sh_{11} &= \oavg{s,t} (\fh_t - \fb) (\fh_t - \fb)' \K{t-s}, \\
  \Sh_{12} &= \oavg{s,t} (\fh_t - \fb)(\gh_t - \gb)' \K{t-s}, &
  \Sh_{13} &= \oavg{s,t} (\gh_t - \gb)(\gh_t - \gb)'  \K{t-s}.
\end{align*}

Define the MDS covariance estimator:
\begin{align*}
  \Sigmah_2 &= \Sh_{21} + \Sh_{22} + \Sh_{22}' + 2 \, \Sh_{23}, &
  \Sh_{21} &= \oavg{t} (\fh_t - \fb) (\fh_t - \fb)', \\
  \Sh_{22} &= \oavg{t} (\fh_t - \fb)(\gh_t - \gb)', &
  \Sh_{23} &= \oavg{t} (\gh_t - \gb)(\gh_t - \gb)'.
\end{align*}

\begin{lem}\label{lem:2}
  Under the conditions of Thoerem~\ref{res:1}, $\Sigmah_1 \to^p
  \Sigma$. If, in addition, $y_t - x_t'\btrue$ is an MDS with respect
  to $\Fs_t$ then $\Sigmah_2 \to^p \Sigma$.
\end{lem}

\begin{rem}
  The statistic we present tests the null hypothesis that the forecast
  errors from the population version of the benchmark model are a
  martingale difference sequence.  This hypothesis may not be
  appropriate, depending on the loss function or utility function of
  interest.  If one wants to test the less restrictive null hypothesis that
  $y_{t} - \yh_{0t}$ is uncorrelated with $\yh_{1t}$ but not
  necessarily an \mds, one can replace $\Sh_{21}$, $\Sh_{22}$
  and $\Sh_{23}$ with their \hac\ counterparts.
  Our statistic can also be modified to test implications of
  optimal forecasts under other loss functions
  \citep[see][]{PaT:07,PaT:07b}; the statistic should be expressed as
  a forecast encompassing test using the models' generalized forecast
  errors.%
\footnote{See \citet{HLN:98} and \citet[Section~4]{ClW:07}.} %
  Again, Lemma~\ref{res:a5} can cover these other applications.
\end{rem}

\section{Monte Carlo Results}\label{sec:2}
This section presents Monte Carlo experiments demonstrating that
this paper's modified version of \citepos{ClW:07} statistic performs
similarly to their original test in the situations they study, but can
have substantially higher power when the \dgp\ has a structural
break.%
\footnote{All of these simulations were programmed in R
  \citep[version 2.14.0]{R} and use the MASS
  \citep[7.3-22]{VeR:02} package.} %

The \dgp\ has three different parametrizations: one to study the
tests' size, one to study power under stationarity, and one to study
power if there is a single break in the relationship between the
target and predictors.  The \dgp\ is:
\begin{align*}
  y_t &= \gamma_{1t} + \gamma_{2t} z_{t-1} + e_t &
  \gamma_t &=
  \begin{cases}
    (0.5, 0)    & \text{size simulations} \\
    (0.5, 0.35) & \text{power (stable)} \\
    (-0.5, 0)    & t \leq \tfrac{T}{2} \quad \text{power (break)} \\
    (1, 0.35) & t > \tfrac{T}{2} \quad \text{power (break)}
  \end{cases}\\\nonumber
  z_t &= 0.15 + 0.95 z_{t-1} + v_t &
  (e_t, v_t)' &\sim iid\ N\Bigg(\begin{pmatrix} 0 \\ 0
  \end{pmatrix}
   , \begin{pmatrix} 18 & -
    0.5 \\ -0.5 & 0.025 \end{pmatrix}\Bigg)
  \\ R &= 120, 240 & P &= 120, 240, 360, 720.
\end{align*}
Both models are estimated by \ols. The benchmark model regresses $y_t$
on a constant, and the alternative regresses $y_t$ on a constant and
$z_{t-1}$.  \citet{ClW:07} argue that this \dgp\ mimics an asset
pricing application similar to \citepos{GoW:08} which we study in
Section~\ref{sec:3}.

For comparison, we study this paper's new statistic as well as \poscw\
rolling-window and recursive-window test statistics.  Clark and West
only prove that their rolling-window statistic is asymptotically
normal, and only then if the benchmark model is not estimated, but
their recursive-window statistic is popular in practice and in
simulations tends to perform similarly to their rolling window test.
We use all three of these statistics to test the null that the
benchmark model's innovation is an \mds.%
\footnote{\citet{ClW:07}
  report the performance of the tests proposed by \citet{CCS:01} and
  \citet{ClM:05} as well, and of tests based on the naive Gaussian
  statistic.} %

\begin{table}[tb]
  \centering
  \input{tex/mc1}
  \caption{Size and power of the \oos\ tests in the simulations 
    described by Section~\ref{sec:2}, at
    \testsize\% confidence.  These percentages are calculated from \totalsims\
    samples.  Pr[CW roll.] shows the fraction of simulations for
    which Clark and West's (2007) rolling-window statistic rejects; 
    Pr[CW rec.] shows the fraction of simulations for which
    their recursive-window statistic rejects; and Pr[new] shows the fraction of
    simulations for which this paper's test rejects.}
\label{tab:mc1}
\end{table}

Table~\ref{tab:mc1} presents the simulation results.  For all of the
stable parameter values, the proposed new statistic has similar
rejection probability to \citepos{ClW:07}.  Both of Clark and West's
tests are generally slightly undersized relative to our new test,
which is itself slightly undersized: when $R$ is 120 and $P$ is 360
our test statistic has size 7.6\% and Clark and West's rolling and
recursive window tests have size 7.5\% and 6.2\% respectively, at a
nominal size of 10\%.  For the stable alternative, our new statistic
typically has slightly higher power than Clark and West's rolling
window and lower power than their recursive window.  For example, when
$R$ is 120 and $P$ is 720, the rolling-window test rejects at 66.8\%,
our statistic at 73.0\%, and the recrusive window statistic at 82.3\%,
again for a nominal size of 10\%.  In general, the statistics perform
similarly under stability.

For the simulations with a single break, the new statistic has
considerably higher power than \poscw\ original tests across all of
the choices of $R$ and $P$; the rejection probability is more than
twice as large for most parametrizations.  When $R$ is 120 and $P$ is
360 with a nominal size of 10\%, for example, the new statistic
rejects at 96.4\% while the rolling and recursive window statistics
reject at 35.5\% and 32.9\% respectively.  Results for other choices
of nominal size and sample split give similar results.  So mixing
window strategies can give a large power advantage when testing for
time-varying predictability, and performs similarly to the original
test when testing for stable outperformance.

\section{Empirical Illustration}\label{sec:3}

This section demonstrates the use of our new statistic by revisiting
\citepos{GoW:08} study of excess stock returns.  Goyal and Welch argue
that many variables thought to predict excess returns (measured as the
difference between the yearly log return of the S\&P 500 index and the
T-bill interest rate) on the basis of in-sample evidence fail to do so
out-of-sample.  To show this, Goyal and Welch look at the forecasting
performance of models using a lag of the variable of interest, and
show that these models do not significantly outperform the excess
return's recursive sample mean.

Here, I conduct the same analysis, but using this paper's \mds\ test.
The benchmark model is the excess return's sample mean (as in the
original) and the alternative models are of the form
\[\text{excess return}_{t} = \alpha_{0} + \alpha_{1}\ 
\text{predictor}_{t-1} + \ep_{t},\] where $\alpha_{0}$ and
$\alpha_{1}$ are estimated by \ols\ using a \windowlength-year window.
The predictors used are listed in the ``predictor'' column of
Table~\ref{tab:em1} \citep[see][for a detailed description of the
variables]{GoW:08}.  We also consider \citepos{CaT:08} proposed
correction to the models, that the forecasts be bounded below by zero
since negative forecasts are incredible, as well as two simple
combination forecasts, the mean and the median (over both the original
and the non-negative forecasts).  The data set is annual data
beginning in 1927 and ending in 2009, and the rolling window uses
\windowlength\ observations.%
\footnote{This statistical analysis was conducted in R \citep{R} and
  uses the MASS \citep[7.3-22]{VeR:02} package.} %

\begin{table}[tb!]
  \centering
  \empiricaltable
\caption{Results from \oos\ comparison of equity premium prediction
  models; the benchmark is the recursive sample mean of the equity
  premium and each alternative model is a constant and single lag of
  the variable listed in the ``predictor'' column.  The dataset begins
  in 1927 and ends in 2009 and is annual data. The ``value'' column
  lists the value of this paper's \oos\ statistic, the ``naive''
  column indicates whether the statistic is significant at standard
  critical values, and the ``corrected'' column indicates significance
  using the critical values proposed in Theorem~\ref{res:2} that
  account for the number of models.  See Section~\ref{sec:3} for details.}
\label{tab:em1}
\end{table}

Table~\ref{tab:em1} presents the results for each model.  The column
``value'' gives the value of the test statistic for each model, while
the ``naive'' indicates whether the statistic is greater than the
standard \bootsize\% critical value (\naivecriticalvalue) Three
predictors are
significant at the naive critical values for both the original and
bounded forecasts: the dividend yield, long term interest rate, and
book to market ratio.

However, we know that this is an extremely optimistic assessment of
the models' performance. We are conducting \nmod\ simultaneous
hypothesis tests, so it is likely that some will reject by chance. A
full treatment of this issue is beyond the scope of this paper,
however, it is straightforward to use Theorem~\ref{res:1} and
Lemma~\ref{lem:2} to derive an appropriate critical value as
in~\citet{Whi:00}. Since $\sqrt{P} \fb \to^d N(0, \Sigma)$ under
the MDS null hypothesis, the continuous mapping theorem implies that
$\max_i \sqrt{P} \fb_i/\sh_i \to^d \max_i W_i$, where $Z_i
\sim N(0, D^{-1/2} \Sigma D^{-1/2})$ and $D = \diag(\sigma_1, \dots,
\sigma_j)$. We can simulate from the $N(0, \hat D^{-1/2} \Sh
\hat D^{-1/2})$ distribution to generate a critical value for the
\emph{largest} individual test statistic under the null ---
\empiricalcriticalvalue, based on \empiricaldraws\ simulations. Using
this critical value, none of the predictors are siginificant at
\bootsize\%.%
\footnote{\citet{Han:05} makes the point that multiple one-sided
  comparisons can have poor power if irrelevant predictors are
  included in these tests and proposes a threshold for discarding very
  poor forecasts. His threshold is well below our worst performing
  model, so this issue is not a concern here. Moreover, I need to
  explain why it's not a concern for these statistics in general.} %

\section{Conclusion}\label{sec:4}
This paper presents an \oos\ test statistic similar to \poscw\ that is
asymptotically normal when comparing nested or non-nested models.
Normality is achieved by estimating the alternative model using a
fixed-length rolling window---as do Clark and West---but estimating
the benchmark model with a recursive window.  Simulations indicate
that the new statistic behaves similarly to Clark and West's original
test when the \dgp\ is stable but can have much higher power when the 
\dgp\ has structural breaks.

\appendix
\section{Proofs of Main Theoretical Results}
\label{sec:B}

\newcommand{\WesA}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) B^{#1} H_t^{#1}}
\newcommand{\WesB}[1][]{\tfrac{1}{\sqrt{P}} \E^{#1} F_t^{#1} \osum{t} (B_t^{#1} -
  B^{#1}) H_t^{#1}}
\newcommand{\WesC}[1][]{\oclt{t}
  (F_t^{#1} - \E^{#1} F_t^{#1}) (B_t^{#1} - B^{#1}) H_t^{#1}}

Define the additional notation 
$F_t(\beta) = \tfrac{\partial}{\partial \beta} f_t(\beta)$,
$F_t = F_t(\btrue)$,
and
$h_t = h_t(\btrue)$.
    
\begin{proof}[Proof of Theorem \ref{res:1}]
  Replace $R$ with $\log(T)$ and $P$ with $T - \log(T)$ in the
  statistic; this substitution does not affect its asymptotic
  distribution.
  As in \citet{Wes:96} and \citet{WeM:98}, expand $\fh_t$ around
  $\btrue$ to get
  \begin{align*}
    \sqrt{P} \big(\fb - \fb(\btrue)\big) &= \oclt{t}
    \big(f_t - \E f_t\big) +
    \E F_t B \oclt{t} H_t \\
    & \quad + \WesA + \WesB \\ & \quad + \WesC + \oclt{t} w_t
  \end{align*}
  where (again, as in \citealp{Wes:96}) the $i$th element of $w_t$ is
  \begin{equation*}
    w_{it} = \tfrac12 (\bh_t - \btrue)'
    \Big[\tfrac{\partial^2}{\partial \beta \partial\beta'}
    f_{it}(\bt_{it}) \Big]
    (\bh_t - \btrue)
  \end{equation*}
  and each $\bt_{it}$ lies between $\bh_t$ and
  $\btrue$; $\oclt{t} w_t = o_{p}(1)$ as in Theorem~\ref{res:3}.
  Then, as before,
  \begin{gather}
    \WesA \to^{p} 0 \label{eq:11} \\
    \WesB \to^{p} 0 \label{eq:12}\\
  \intertext{and}
    \WesC \to^{p} 0 \label{eq:13}
  \end{gather}
  from Lemma~\ref{res:a4}.  The formula of the asymptotic variance can
  be derived exactly as in \citet{Wes:96} and \citet{WeM:98}.

  Note that \[f_t(\btrue)= 2 (y_{t+1} -
  x_t'\btrue)(\yh_{1,t+1} - x_t'\btrue) \quad \text{a.s.},\] as
  in \citet{ClW:07}, which is an \mds, so we do not need to use a
  \hac\ estimator of the variance under the null.  Also observe that
  \[F_t(\beta) = 2 x_t(x_t'\beta - \yh_{1,t+1}) + 2 x_t(x_t'\beta -
  y_{t+1})\] and the second term is an \mds\ under the null,
  explaining the particular form of $g_t(\beta)$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{res:2}]
  Replace $R$ and $P$ as in the proof of Theorem~\ref{res:1}.
  Theorem~\ref{res:3} of this paper and Theorems~3.1 and~4.1 of
  \citet{RoW:05} complete the proof.
\end{proof}

\section{Supporting Results}

\begin{lema}\label{res:a2}
  Suppose $a \in [0,\frac12)$ and the conditions of Theorem~\ref{res:3}
  hold.
  \begin{enumerate}
  \item $P^a \sup_t | H_{t} | \to^p 0$.
  \item $P^a \sup_t | \bh_{t} - \btrue | \to^{p} 0$.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t) = O_{p}(1)$.
  \end{enumerate}
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a2}]
  \begin{enumerate}
  \item The process $\tfrac{1}{\sqrt{T}} h_{s}$ satisfies
    \citepos[Theorem 3.1]{JoD:00b} functional \clt.  So
    \begin{equation}
      P^a \sup_t \Big| \tfrac1t \sum_{s=1}^t h_{s} \Big| =
      P^a \sup_{\gamma \in [0,1]} \Big| \tfrac{1}{\lfloor \gamma
        T\rfloor} \sum_{s=1}^{\lfloor \gamma T \rfloor} h_{s} \Big| \to^{p} 0
    \end{equation}
    with the convergence following from the continuous mapping
    theorem.
  \item We have
    \begin{equation}
      P^a \sup_t | \bh_t - \btrue | = P^a \sup_t |\Bh_{t}
      H_{t}| \leq \sup_{t,u} \Big| [ \Bh_u - B]
      \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big| + \sup_{t} \Big|
      B \tfrac{P^a}{t} \sum_{s=1}^t h_{s} \Big|
    \end{equation}
    and both terms converge to zero in (conditional) probability by
    the previous argument and by assumption.
  \item $\tfrac{1}{\sqrt{P}} \sum_{t=R+1}^T (F_t - \E F_t)$ obeys
    \citepos{Jon:97} \clt, so the result is trivial.
  \end{enumerate}
\end{proof}

\begin{lema}\label{res:a4}
  Under the conditions of Theorem~\ref{res:1}, Equations
  \eqref{eq:11}--\eqref{eq:13} hold.
\end{lema}

\begin{proof}[Proof of Lemma~\ref{res:a4}]
We can write
\begin{equation*}
  \Big\lvert \WesA \Big\rvert \leq 
  \Big\lvert \oclt{t} (F_t - \E F_t) B \Big\rvert
  \sup_t | H_t |.
\end{equation*}
From Lemma~\ref{res:a2}, $\sup_t | H_t | \to^p 0$ and $\oclt{t}
(F_t - \E F_t) = O_p(1)$, establishing~\eqref{eq:11}. The proofs
of \eqref{eq:12} and \eqref{eq:13} are similar.
\end{proof}

\bibliography{texextra/AllRefs}
\end{document}

% LocalWords:  ClW JEL ISI Google GiW Mcc ClM CoS CCS StW IMA GiR WeM fh X'X hh
% LocalWords:  PaT AllRefs isi ima uc sv PeT GoW lm il GoK RoW Econometrica PoR
% LocalWords:  Finan StepM studentizing studentization Whi HuW RSW recentering
% LocalWords:  DiM LiS Kun McCracken lt filtrations GoJR JoD McCracken's Econom
% LocalWords:  Corradi unstudentized studentized GoJ gcalhoun HLN li PoW
% LocalWords:  Econometricians reestimate PPW resample miscentered AnG eq Helle
% LocalWords:  Bunzel Yu Hsu Pincheira HHK th AtO ik DoH Wolak's Wol stepdown
% LocalWords:  Hsu's iq mk Ames Amit Goyal rfs Ivo Welch Wes covariance MeR McW
% LocalWords:  familywise prespecified CoD Gia pointwise misspecified InK MeP
% LocalWords:  VeR xtable Dah dbframe oos parametrizations iid dgp return's CaT
% LocalWords:  outperformance Hmisc Har nondifferentiable bT jt Meng
% LocalWords:  stationarity mixingale mixingales texextra Timmermann